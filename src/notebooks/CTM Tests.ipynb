{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.random as rd\n",
      "import scipy as sp\n",
      "import scipy.sparse as ssp\n",
      "import scipy.linalg as la\n",
      "import scipy.sparse.linalg as sla\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "\n",
      "from math import log, pi, exp\n",
      "\n",
      "rd.seed(0xBADB055)\n",
      "\n",
      "%load_ext cythonmagic\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize_rows_ip(matrix):\n",
      "    '''\n",
      "    Normalizes the rows of the matrix in-place, ovewriting\n",
      "    the previous values\n",
      "    '''\n",
      "    row_sums  = matrix.sum(axis=1)\n",
      "    matrix   /= row_sums[:, np.newaxis]\n",
      "    return matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Artifical Dataset\n",
      "## Vocabularies\n",
      "We create six hard-coded vocabularies, each of which can easily be visualized as $3 \\times 3$ square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T = 3 * 3\n",
      "K = 6\n",
      "\n",
      "# Horizontal bars\n",
      "vocab1 = ssp.coo_matrix(([1, 1, 1], ([0, 0, 0], [0, 1, 2])), shape=(3,3)).todense()\n",
      "vocab2 = ssp.coo_matrix(([1, 1, 1], ([1, 1, 1], [0, 1, 2])), shape=(3,3)).todense()\n",
      "vocab3 = ssp.coo_matrix(([1, 1, 1], ([2, 2, 2], [0, 1, 2])), shape=(3,3)).todense()\n",
      "\n",
      "# Vertical bars\n",
      "vocab4 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [0, 0, 0])), shape=(3,3)).todense()\n",
      "vocab5 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [1, 1, 1])), shape=(3,3)).todense()\n",
      "vocab6 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [2, 2, 2])), shape=(3,3)).todense()\n",
      "\n",
      "# Diagonals\n",
      "vocab7 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [0, 1, 2])), shape=(3,3)).todense()\n",
      "vocab8 = ssp.coo_matrix(([1, 1, 1], ([2, 1, 0], [0, 1, 2])), shape=(3,3)).todense()\n",
      "\n",
      "# Put together\n",
      "T = vocab1.shape[0] * vocab1.shape[1]\n",
      "vocabs = [vocab1, vocab2, vocab3, vocab4, vocab5, vocab6] #, vocab7, vocab8]\n",
      "\n",
      "# Create a single matrix with the flattened vocabularies\n",
      "vocabVectors = []\n",
      "for vocab in vocabs:\n",
      "    vocabVectors.append (np.squeeze(np.asarray (vocab.reshape((1,T)))))\n",
      "\n",
      "vocab = normalize_rows_ip(np.array(vocabVectors, dtype=np.float32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ones = np.ones(vocabs[0].shape)\n",
      "for k in range(K):\n",
      "    plt.subplot(2, 3, k)\n",
      "    plt.imshow(ones - vocabs[k], interpolation=\"none\", cmap = cm.Greys_r)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD9CAYAAAC7iRw+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x106720610>"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Topic Distributions and the Document-Term Matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rd.seed(0xC0FFEE)\n",
      "D = 100\n",
      "\n",
      "topicDist = normalize_rows_ip (rd.random((D,K)))\n",
      "\n",
      "meanWordCount = 20\n",
      "wordCounts = rd.poisson(meanWordCount, size=D)\n",
      "W = np.floor(topicDist.dot(vocab) * wordCounts[:, np.newaxis])\n",
      "\n",
      "Ws = ssp.csr_matrix(W)\n",
      "print (\"Non-zero count is \\n\" + str(Ws.nonzero))\n",
      "\n",
      "print (\"\\n\\n Matrix\" + str(W[0:10,:]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Non-zero count is \n",
        "<bound method csr_matrix.nonzero of <100x9 sparse matrix of type '<class 'numpy.float64'>'\n",
        "\twith 834 stored elements in Compressed Sparse Row format>>\n",
        "\n",
        "\n",
        " Matrix[[ 1.  1.  2.  1.  2.  3.  1.  2.  3.]\n",
        " [ 1.  1.  1.  1.  1.  0.  2.  2.  2.]\n",
        " [ 2.  2.  1.  3.  3.  2.  3.  3.  2.]\n",
        " [ 1.  1.  1.  1.  1.  2.  1.  1.  2.]\n",
        " [ 2.  1.  1.  1.  0.  1.  2.  1.  2.]\n",
        " [ 0.  0.  0.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  3.  2.  2.  2.  2.  2.]\n",
        " [ 1.  2.  1.  1.  2.  1.  1.  2.  1.]\n",
        " [ 2.  2.  2.  1.  1.  2.  2.  2.  3.]\n",
        " [ 3.  3.  3.  2.  3.  2.  2.  2.  2.]]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the full list of topic distributions for all $D$ documents presented in percentage form."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(topicDist * 100).astype(np.int32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([[15, 22, 25,  2,  7, 26],\n",
        "       [12,  0, 32, 21, 21, 11],\n",
        "       [11, 20, 20, 18, 24,  5],\n",
        "       [17, 26, 22,  6,  9, 18],\n",
        "       [14,  1, 24, 27, 10, 21],\n",
        "       [10, 22, 22, 15, 14, 15],\n",
        "       [ 4, 28, 20, 19, 13, 14],\n",
        "       [13, 24, 15,  8, 27, 10],\n",
        "       [23, 15, 31,  7,  6, 15],\n",
        "       [19, 13, 10, 17, 20, 18],\n",
        "       [28, 15, 22,  5, 10, 17],\n",
        "       [11, 28,  3, 18, 15, 21],\n",
        "       [ 3, 18, 35,  5,  8, 29],\n",
        "       [ 7, 23,  8, 10, 27, 21],\n",
        "       [ 8, 27, 19, 24, 10,  8],\n",
        "       [30, 13, 12, 16, 12, 14],\n",
        "       [ 3, 17, 16, 27, 20, 14],\n",
        "       [18, 13, 12, 20, 18, 16],\n",
        "       [ 5, 14, 28, 27, 22,  2],\n",
        "       [17, 20, 10,  1, 25, 24],\n",
        "       [18, 17, 24,  3, 24, 11],\n",
        "       [ 4,  0, 18, 27, 28, 20],\n",
        "       [13, 28,  8, 19, 17, 12],\n",
        "       [ 2, 13, 26, 23,  6, 26],\n",
        "       [29,  0, 24,  7,  4, 32],\n",
        "       [62,  7, 11,  2, 12,  4],\n",
        "       [ 9, 17, 28, 13,  1, 28],\n",
        "       [24,  1,  4, 15, 28, 25],\n",
        "       [17,  5, 32,  9,  3, 30],\n",
        "       [ 3, 18, 30,  6, 23, 18],\n",
        "       [12, 34,  6, 27,  7, 10],\n",
        "       [ 8,  2, 26, 12, 27, 21],\n",
        "       [15, 27,  5,  8, 19, 24],\n",
        "       [21,  2, 16, 30,  8, 19],\n",
        "       [12, 16,  6, 23, 11, 29],\n",
        "       [22, 13, 18,  6, 14, 25],\n",
        "       [14, 23, 12, 10, 15, 23],\n",
        "       [12, 20, 28, 12,  1, 24],\n",
        "       [24, 11, 15, 11, 21, 16],\n",
        "       [ 1, 15, 31, 14,  9, 28],\n",
        "       [16, 22, 38,  8, 13,  0],\n",
        "       [21, 15,  4, 24, 16, 17],\n",
        "       [26,  8, 37,  1, 21,  4],\n",
        "       [ 3, 21, 16, 23, 24, 10],\n",
        "       [29, 18,  1, 21,  4, 24],\n",
        "       [27, 30,  3, 28,  7,  2],\n",
        "       [17,  6, 26, 14, 26,  8],\n",
        "       [33, 32, 10,  0,  4, 18],\n",
        "       [16, 25, 28,  0, 26,  2],\n",
        "       [ 4, 24, 24, 20, 17,  8],\n",
        "       [25, 25, 25,  5,  3, 13],\n",
        "       [22,  4, 12, 23, 17, 18],\n",
        "       [15, 24,  6, 19, 14, 19],\n",
        "       [ 9, 20,  9, 12, 23, 24],\n",
        "       [29, 30,  7, 15,  9,  8],\n",
        "       [18, 14, 12, 30,  3, 20],\n",
        "       [10,  6, 10, 29, 29, 13],\n",
        "       [ 9, 19, 14, 23, 12, 20],\n",
        "       [28,  3, 20, 10, 22, 13],\n",
        "       [22, 25,  0, 24, 23,  2],\n",
        "       [14, 22, 23, 20, 13,  6],\n",
        "       [21, 19, 16, 20, 16,  6],\n",
        "       [21, 18,  3,  4, 27, 24],\n",
        "       [14,  8, 15, 22, 29,  8],\n",
        "       [19, 10, 29, 12, 14, 13],\n",
        "       [ 8, 33,  8, 11, 22, 15],\n",
        "       [10, 27, 22, 10, 29,  0],\n",
        "       [ 6, 23, 30,  1, 10, 27],\n",
        "       [13, 22,  8, 21, 12, 21],\n",
        "       [ 7, 24, 12, 44,  0, 11],\n",
        "       [12, 22, 24,  7, 28,  5],\n",
        "       [25, 24,  1,  7, 15, 24],\n",
        "       [19, 28, 15,  6, 24,  5],\n",
        "       [22,  7, 19, 21, 15, 13],\n",
        "       [ 9, 23, 16, 20,  1, 28],\n",
        "       [31, 14, 15, 18,  3, 15],\n",
        "       [ 9, 24, 23,  7, 24, 10],\n",
        "       [23, 10, 15,  8, 11, 29],\n",
        "       [15, 36,  4, 32,  6,  5],\n",
        "       [ 7, 19, 24, 13, 30,  3],\n",
        "       [31,  4, 21,  9, 28,  3],\n",
        "       [13, 11, 18, 21, 19, 16],\n",
        "       [34,  0,  1, 28, 18, 16],\n",
        "       [27, 30,  4, 24,  8,  5],\n",
        "       [32,  6,  4, 11, 25, 19],\n",
        "       [29, 10, 15, 31,  8,  3],\n",
        "       [20,  3, 18, 10, 34, 11],\n",
        "       [26, 24, 22, 14,  1, 10],\n",
        "       [16,  2, 28, 10,  9, 31],\n",
        "       [19, 28, 19, 24,  2,  4],\n",
        "       [30,  2,  2,  3, 29, 31],\n",
        "       [18, 22, 14, 11, 13, 19],\n",
        "       [ 9, 21, 21, 21, 19,  6],\n",
        "       [ 2, 21, 20, 19, 17, 19],\n",
        "       [ 6,  8, 21, 19, 25, 18],\n",
        "       [24, 34, 12,  8, 10,  9],\n",
        "       [19, 24,  0, 24, 21,  9],\n",
        "       [22, 20,  4, 10, 21, 20],\n",
        "       [12,  8,  8, 55,  4,  9],\n",
        "       [21, 21,  7, 14, 15, 20]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(vocab * 100).astype(np.int32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([[33, 33, 33,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0, 33, 33, 33,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0, 33, 33, 33],\n",
        "       [33,  0,  0, 33,  0,  0, 33,  0,  0],\n",
        "       [ 0, 33,  0,  0, 33,  0,  0, 33,  0],\n",
        "       [ 0,  0, 33,  0,  0, 33,  0,  0, 33]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordCounts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "array([21, 16, 25, 15, 16, 11, 21, 15, 21, 27, 23, 20, 12, 21, 18, 28, 19,\n",
        "       14, 28, 26, 20, 18, 18, 18, 18, 17, 24, 15, 17, 25, 14, 21, 25, 32,\n",
        "       26, 14, 24, 24, 28, 24, 26, 19, 21, 19, 27, 17, 19, 18, 13, 17, 14,\n",
        "       21, 19, 21, 21, 19, 23, 12, 26, 19, 27, 26, 27, 24, 15, 21, 21, 22,\n",
        "       17, 22, 19, 28, 18, 15, 27, 15, 22, 15, 18, 26, 23, 17, 18, 12, 21,\n",
        "       21, 20, 15, 21, 24, 28, 21, 24, 19, 25, 18, 21, 20, 23, 14])"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Proof of Vectorizations\n",
      "\n",
      "## Bound on the topic means\n",
      "\n",
      "$\\sum_d \\left[  -\\frac{K}{2} \\ln 2\\pi - \\frac{1}{2}\\ln|\\Sigma| - \\frac{1}{2} \\left( (m_d - \\mu)^{\\top} \\Sigma^{-1} (m_d - \\mu) \\right) + \\text{tr}(V_d \\Sigma^{-1})\\right]$\n",
      "\n",
      "where $V_d$ is the diagaonal variance associated with the posterior mean."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu   = rd.random((K,))\n",
      "mu  /= np.sum(mu)\n",
      "sigT = rd.random((K,K)) * 2\n",
      "sigT = sigT.T.dot(sigT)\n",
      "M    = normalize_rows_ip(rd.random((D,K)))\n",
      "V    = rd.random((D,K))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Scalar\n",
      "sca = 0\n",
      "for d in range(D):\n",
      "    sca -= K/2. * np.log(2*pi)\n",
      "    sca -= 1/2. * la.det(sigT)\n",
      "    sca -= 1/2. * (M[d,:] - mu).T.dot(la.inv(sigT)).dot(M[d,:] - mu)\n",
      "    sca -= 1/2. * np.trace(np.diag(V[d,:]).dot(la.inv(sigT)))\n",
      "\n",
      "print(\"%f\" % sca)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-4377.569735\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Vectorization\n",
      "vec = 0\n",
      "\n",
      "vec -= (D*K)/2. * np.log(2*pi)\n",
      "vec -= D/2. * la.det(sigT)\n",
      "dif = M - mu[np.newaxis,:]\n",
      "vec -= 0.5 * (np.sum (dif.dot(la.inv(sigT)) * dif))\n",
      "vec -= 0.5 * np.sum(V * np.diag(la.inv(sigT))[np.newaxis,:]) # = sum_d tr(V_d \\Sigma^{-1})\n",
      "             \n",
      "print(\"%f\" % vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-4377.569735\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bound on Approx to Z_dvk\n",
      "\n",
      "This is a bit of a horror show, as it takes into account several variables in the approximation.\n",
      "\n",
      "First we define a couple of useful functions, and create all the variables that we need. This includes the memberships $Z \\in \\mathbb{R}^{D \\times T \\ \\times K}$. \n",
      "\n",
      "In the vectorized form, $Z$ no longer exists, as we've subsituted in the solution for $Z$ directly into all the equations, so we don't have to materialize such an enormous tensor "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LOG_ZERO = log(1E-300)\n",
      "\n",
      "def safe_log(x):\n",
      "    return log(max(x, 1E-300))\n",
      "\n",
      "def np_safe_log(X):\n",
      "    out = np.ndarray(X.shape)\n",
      "    out.fill (LOG_ZERO)\n",
      "    out[X > 1E-300] = np.log(X[X > 1E-200])\n",
      "    return out\n",
      "    \n",
      "\n",
      "def negJakkola(vec):\n",
      "    '''\n",
      "    The negated version of the Jakkola expression which was used in Bouchard's NIPS\n",
      "    2007 softmax bound\n",
      "    \n",
      "    CTM Source reads: y = .5./x.*(1./(1+exp(-x)) -.5);\n",
      "    '''\n",
      "    \n",
      "    #\u00a0COPY AND PASTE BETWEEN THIS AND negJakkolaOfDerivedXi()\n",
      "    return 0.5/vec * (1./(1 + np.exp(-vec)) - 0.5)\n",
      "\n",
      "def deriveXi (means, varcs, s):\n",
      "    '''\n",
      "    Derives a value for xi. This is not normally needed directly, as we\n",
      "    normally just work with the negJakkola() function of it\n",
      "    '''\n",
      "    return np.sqrt(means**2 - 2 * means * s[:,np.newaxis] + (s**2)[:,np.newaxis] + varcs**2)   \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s   = rd.random((D,))\n",
      "xi  = deriveXi(M, V, s)\n",
      "lxi = negJakkola(xi)\n",
      "eM  = np.exp(M)\n",
      "\n",
      "z = np.ndarray(shape=(D,T,K))\n",
      "z.fill(0)\n",
      "\n",
      "for d in range(D):\n",
      "    for v in range(T):\n",
      "        denom = 0\n",
      "        for j in range(K):\n",
      "            denom += vocab[j,v] * eM[d,j]\n",
      "        for k in range(K):\n",
      "            z[d,v,k] = vocab[k,v] * eM[d,k] / denom\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gT = np.exp(M)     # D x T\n",
      "gU = gT.dot(vocab) # D x V\n",
      "gR = W / gU        # D x V\n",
      "gV = gT * (gR.dot(vocab.T)) # D x K\n",
      "\n",
      "vocabLen = vocab.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[sum(W[0,v] * z[0,v,k] for v in range(T)) for k in range(K)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "[1.9619579815730681,\n",
        " 3.0165265937367236,\n",
        " 3.0343909353711593,\n",
        " 1.5738951760976587,\n",
        " 2.4159140002331565,\n",
        " 3.9973153129882335]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gV[0,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "array([ 1.96195798,  3.01652659,  3.03439094,  1.57389518,  2.415914  ,\n",
        "        3.99731531])"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the functions defined and sample values listed, we now compare the scalar and vectorized forms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = wordCounts\n",
      "\n",
      "sca1, sca2, sca3, sca4 = 0, 0, 0, 0\n",
      "\n",
      "# Line 1 from Guillaume's note\n",
      "sca1 -= sum((M[d,k]**2 + V[d,k]**2) * n[d] * lxi[d,k] \\\n",
      "           for k in range(K) for d in range(D))\n",
      "sca1 += sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d] \\\n",
      "            + sum(W[d,v] * z[d,v,k] for v in range(T))) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 2 from Guillaume's note\n",
      "sca2 += sum(safe_log(vocab[k,v]) * (sum (W[d,v] * z[d,v,k] for d in range(D))) for k in range(K) for v in range(T))\n",
      "sca2 -= sum(W[d,v] * sum(z[d,v,k] * safe_log(z[d,v,k]) for k in range(K)) \\\n",
      "           for v in range(T) for d in range(D))\n",
      "\n",
      "# Line 3 from Guillaume's note\n",
      "sca3 -= sum(n[d] * (\\\n",
      "            lxi[d,k]*(s[d]**2 - xi[d,k]**2) \\\n",
      "            - 0.5*(s[d] + xi[d,k]) \\\n",
      "            + log(1 + exp(xi[d,k])) \\\n",
      "            ) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 4 omitted from Guillaume's note\n",
      "# In my derivation it's -sum_d sum_n sum_k z_dnk s_d = -\\sum_d n_d s_d\n",
      "sca4 -= sum(n[d] * s[d] for d in range(D))\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (sca1, sca2, sca3, sca4, sca1+sca2+sca3+sca4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1058.621222 + -646.943452 + -6020.548604 + -981.298486 = -8707.411765\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A Quick test of sums over W and Z"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = 0\n",
      "for d in range(D):\n",
      "    for k in range(K):\n",
      "        for v in range(vocabLen):\n",
      "            sca += W[d,v] * z[d,v,k]\n",
      "\n",
      "vec = np.sum(M.T.dot(Vc))\n",
      "\n",
      "print (\"Scalar = %f, Vectorized = %f\\n\" %(sca, vec))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'Vc' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-36-4f42faaab647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0msca\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Scalar = %f, Vectorized = %f\\n\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'Vc' is not defined"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, once you think about it, it's a weighted sum of the per-topic word-counts per documents, weighted by the topic assignments per document, which is basically the sum of the word-counts over all documents\n",
      "\n",
      "Finally the vectorized form of the bound"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec1, vec2, vec3, vec4 = 0, 0, 0, 0\n",
      "\n",
      "vec1 -= np.sum((M*M + V*V) * n[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * -0.5 * n[:,np.newaxis])\n",
      "vec1 += np.sum(M * gV)\n",
      "\n",
      "vec2 -= np.sum(M * gV) # Note the cancelation above\n",
      "vec2 -= -np.sum(np_safe_log(gU) * W) # R * TB = W\n",
      "\n",
      "vec3 -= np.sum(n[:,np.newaxis] * lxi * ((s*s)[:,np.newaxis] - (xi * xi)))\n",
      "vec3 += np.sum(0.5 * n[:,np.newaxis] * (s[:,np.newaxis] + xi))\n",
      "vec3 -= np.sum(n[:,np.newaxis] * np_safe_log(1 + np.exp(xi)))\n",
      "\n",
      "vec4 -= np.dot(s, n)\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (vec1, vec2, vec3, vec4, vec1+vec2+vec3+vec4))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Breakdown of Line 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum((M*M + V*V) * n[:,np.newaxis] * lxi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum((M[d,k]**2 + V[d,k]**2) * n[d] * lxi[d,k] \\\n",
      "           for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi) \\\n",
      "+ np.sum(M * -0.5 * n[:,np.newaxis]) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d]) for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(M * gV)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(M[d,k] * sum(W[d,v] * z[d,v,k] for v in range(T)) \\\n",
      "            for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi) \\\n",
      "+ np.sum(M * -0.5 * n[:,np.newaxis]) \\\n",
      "+ np.sum(M * gV)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "-509.94645438751496"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d] \\\n",
      "              +sum(W[d,v] * z[d,v,k] for v in range(T))) \\\n",
      "              for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "-509.9464543875149"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Breakdown of Line 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca21 = sum(safe_log(vocab[k,v]) * (sum (W[d,v] * z[d,v,k] for d in range(D))) for k in range(K) for v in range(T))\n",
      "sca22 = -sum(W[d,v] * sum(z[d,v,k] * safe_log(z[d,v,k]) for k in range(K)) \\\n",
      "           for v in range(T) for d in range(D))\n",
      "\n",
      "sca21, sca22"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "(-1740.2018180434079, 1093.2583656058923)"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(np_safe_log(vocab) * vocab * gT.T.dot(gR))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "-1740.2018966674805"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec21 = np.sum(np_safe_log(vocab) * vocab * gT.T.dot(gR))\n",
      "vec22 = np.sum(M * gV)\n",
      "vec23 = -np.sum(np_safe_log(gU) * gR * gT.dot(vocab))\n",
      "\n",
      "print (\"%f + %f + %f = %f\\n\" % (vec21, vec22, vec23, vec21 + vec22 + vec23))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1740.201897 + 275.776541 + 371.166912 = -1093.258444\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bounds on Z_dvk Using Sparse Computations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##\u00a0Cython Helpers\n",
      "The matrix $U$ is a _dense_ $D \\times T$ matrix. However it is always used in conjunction with the _sparse_ $D \\times T$ document-term matrix $W$. Consequently, there is a massive performance improvement to be gained by only calculating the values of $U$ for the non-zero elements of $W$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython\n",
      "\n",
      "cimport cython\n",
      "import numpy as np\n",
      "cimport numpy as np\n",
      "from libc.math cimport log\n",
      "from libc.float cimport FLT_MIN, DBL_MIN\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def sparseScalarQuotientOfDot_f8(double[:] A_data, int[:] A_indices, int[:] A_ptr, double[:,:] B, double[:,:] C, double[:] out_data):\n",
      "    '''\n",
      "    Returns A / np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A_data    - the values buffer of the sparse CSR matrix A\n",
      "    A_indices - the indices buffer of the sparse CSR matrix A\n",
      "    A_ptr     - the index pointer buffer of the sparse CSR matrix A\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out_data  - the values buffer into which the result will be placed.\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    cdef int rowCount = len(A_ptr) - 1 \n",
      "    cdef int elemCount = 0, e = 0\n",
      "    cdef int row = 0, col = 0, i = 0\n",
      "    with nogil:\n",
      "        while row < rowCount:\n",
      "            elemCount = A_ptr[row+1] - A_ptr[row]\n",
      "            e = 0\n",
      "            while e < elemCount:\n",
      "                col = A_indices[i]\n",
      "                out_data[i] = A_data[i] / dotProduct_f8(row,col,B,C)\n",
      "                i += 1\n",
      "                e += 1\n",
      "            row += 1\n",
      "    \n",
      "    return out_data\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def sparseScalarProductOfDot_f8(double[:] A_data, int[:] A_indices, int[:] A_ptr, double[:,:] B, double[:,:] C, double[:] out_data):\n",
      "    '''\n",
      "    Returns A * np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A_data    - the values buffer of the sparse CSR matrix A\n",
      "    A_indices - the indices buffer of the sparse CSR matrix A\n",
      "    A_ptr     - the index pointer buffer of the sparse CSR matrix A\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out_data  - the values buffer into which the result will be placed.\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    cdef int rowCount = len(A_ptr) - 1 \n",
      "    cdef int elemCount = 0, e = 0\n",
      "    cdef int row = 0, col = 0, i = 0\n",
      "    with nogil:\n",
      "        while row < rowCount:\n",
      "            elemCount = A_ptr[row+1] - A_ptr[row]\n",
      "            e = 0\n",
      "            while e < elemCount:\n",
      "                col = A_indices[i]\n",
      "                out_data[i] = A_data[i] * dotProduct_f8(row,col,B,C)\n",
      "                i += 1\n",
      "                e += 1\n",
      "            row += 1\n",
      "    \n",
      "    return out_data\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def sparseScalarProductOfSafeLnDot_f8(double[:] A_data, int[:] A_indices, int[:] A_ptr, double[:,:] B, double[:,:] C, double[:] out_data):\n",
      "    '''\n",
      "    Returns A * np.log(np.dot(B, C)), however it does so keeping in \n",
      "    mind the sparsity of A, calculate values only when required.\n",
      "    Moreover if any product of the dot is zero, it's replaced with\n",
      "    the minimum non-zero value allowed by the datatype, to avoid NaNs\n",
      "     \n",
      "    Params\n",
      "    A_data    - the values buffer of the sparse CSR matrix A\n",
      "    A_indices - the indices buffer of the sparse CSR matrix A\n",
      "    A_ptr     - the index pointer buffer of the sparse CSR matrix A\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out_data  - the values buffer into which the result will be placed.\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    cdef int rowCount = len(A_ptr) - 1 \n",
      "    cdef int elemCount = 0, e = 0\n",
      "    cdef int row = 0, col = 0, i = 0\n",
      "    cdef double dotProd = 0.0\n",
      "    cdef double logOfMin = log (DBL_MIN)\n",
      "    \n",
      "    with nogil:\n",
      "        while row < rowCount:\n",
      "            elemCount = A_ptr[row+1] - A_ptr[row]\n",
      "            e = 0\n",
      "            while e < elemCount:\n",
      "                col         = A_indices[i]\n",
      "                dotProd     = dotProduct_f8(row,col,B,C)\n",
      "                out_data[i] = A_data[i] * log(dotProd) if dotProd > DBL_MIN else logOfMin\n",
      "                i += 1\n",
      "                e += 1\n",
      "            row += 1\n",
      "    \n",
      "    return out_data\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "cdef double dotProduct_f8 (int r, int c, double[:,:] B, double[:,:] C) nogil:\n",
      "    '''\n",
      "    The dot product of the r-th row of B and the c-th column of C.\n",
      "    Done directly with a for-loop, no BLAS, SSE or anything. Still\n",
      "    pretty fast though - just as quick as a numpy dot\n",
      "    '''\n",
      "\n",
      "    cdef double result = 0\n",
      "    cdef int innerDim = B.shape[1]\n",
      "    \n",
      "    cdef int i = 0\n",
      "    while i < innerDim:\n",
      "        result += B[r,i] * C[i,c]\n",
      "        i += 1\n",
      "\n",
      "    return result\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import autojit\n",
      "\n",
      "def sparseScalarQuotientOfDot (A, B, C, out=None):\n",
      "    '''\n",
      "    Returns A / np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A         - a sparse CSR matrix\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out       - if specified, must be a sparse CSR matrix with identical\n",
      "                non-zero pattern to A (i.e. same indices and indptr)\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "        \n",
      "    if A.dtype == np.float64:\n",
      "        sparseScalarQuotientOfDot_f8(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    elif A.dtype == np.float32:\n",
      "        sparseScalarQuotientOfDot_f4(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    else:\n",
      "        _sparseScalarQuotientOfDot_py(A,B,C, out)\n",
      "    return out\n",
      "\n",
      "\n",
      "def sparseScalarProductOfDot (A, B, C, out=None):\n",
      "    '''\n",
      "    Returns A * np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A         - a sparse CSR matrix\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out       - if specified, must be a sparse CSR matrix with identical\n",
      "                non-zero pattern to A (i.e. same indices and indptr)\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if A.dtype == np.float64:\n",
      "        sparseScalarQuotientOfDot_f8(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    elif A.dtype == np.float32:\n",
      "        sparseScalarQuotientOfDot_f4(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    else:\n",
      "        _sparseScalarQuotientOfDot_py(A,B,C, out)\n",
      "    return out\n",
      "\n",
      "def _sparseScalarProductOfDot_py(A,B,C, out=None):\n",
      "    '''\n",
      "    Calculates A * B.dot(C) where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based multiplication.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if out is not A:\n",
      "        out.data[:] = A.data\n",
      "    \n",
      "    out.data *= B.dot(C)[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def sparseScalarProductOfSafeLnDot (A, B, C, out=None):\n",
      "    '''\n",
      "    Returns A * np.log(np.dot(B, C)), however it does so keeping in\n",
      "    mind the sparsity of A, calculating values only where required.\n",
      "    Moreover if any product of the dot is zero, it's replaced with\n",
      "    the minimum non-zero value allowed by the datatype, to avoid NaNs\n",
      "     \n",
      "    Params\n",
      "    A         - a sparse CSR matrix\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out       - if specified, must be a sparse CSR matrix with identical\n",
      "                non-zero pattern to A (i.e. same indices and indptr)\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if A.dtype == np.float64:\n",
      "        sparseScalarProductOfSafeLnDot_f8(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    elif A.dtype == np.float32:\n",
      "        sparseScalarProductOfSafeLnDot_f4(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    else:\n",
      "        _sparseScalarProductOfSafeLnDot_py(A,B,C, out)\n",
      "    return out\n",
      "\n",
      "\n",
      "def _sparseScalarProductOfSafeLnDot_py(A,B,C, out=None):\n",
      "    '''\n",
      "    Calculates A * B.dot(C) where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based multiplication.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    out.data[:] = A.data\n",
      "    \n",
      "    rhs = B.dot(C)\n",
      "    rhs[rhs < sys.float_info.min] = sys.float_info.min\n",
      "    out.data *= np.log(rhs)[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def sparseScalarProductOf(A,B, out=None):\n",
      "    '''\n",
      "    Calculates A * B where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based multiplication.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if not out is A:\n",
      "        out.data[:] = A.data\n",
      "    out.data *= B[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def _sparseScalarQuotientOfDot_py(A,B,C, out=None):\n",
      "    '''\n",
      "    Calculates A / B.dot(C) where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based division.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if not out is A:\n",
      "        out.data[:] = A.data\n",
      "    \n",
      "    out.data /= B.dot(C)[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "@autojit\n",
      "def csr_indices(ptr, ind):\n",
      "    '''\n",
      "    Returns the indices of a CSR matrix, given its indptr and indices arrays.\n",
      "    '''\n",
      "    rowCount = len(ptr) - 1 \n",
      "    \n",
      "    rows = [0] * len(ind)\n",
      "    totalElemCount = 0\n",
      "\n",
      "    for r in range(rowCount):\n",
      "        elemCount = ptr[r+1] - ptr[r]\n",
      "        if elemCount > 0:\n",
      "            rows[totalElemCount : totalElemCount + elemCount] = [r] * elemCount\n",
      "        totalElemCount += elemCount\n",
      "\n",
      "    return [rows, ind.tolist()]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##\u00a0Sparse Implementations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = vocab.astype(np.float64)\n",
      "\n",
      "gT = np.exp(M)     # D x T\n",
      "gU = gT.dot(vocab) # D x V\n",
      "gR = sparseScalarQuotientOfDot(Ws, gT, vocab)  # D x V   [W / U]\n",
      "gV = gT * (gR.dot(vocab.T)) # D x K\n",
      "\n",
      "vocabLen = vocab.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The stanard scalar equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = wordCounts\n",
      "\n",
      "sca1, sca2, sca3, sca4 = 0, 0, 0, 0\n",
      "\n",
      "# Line 1 from Guillaume's note\n",
      "sca1 -= sum((M[d,k]**2 + V[d,k]**2) * n[d] * lxi[d,k] \\\n",
      "           for k in range(K) for d in range(D))\n",
      "sca1 += sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d] \\\n",
      "            + sum(W[d,v] * z[d,v,k] for v in range(T))) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 2 from Guillaume's note\n",
      "sca2 += sum(safe_log(vocab[k,v]) * (sum (W[d,v] * z[d,v,k] for d in range(D))) for k in range(K) for v in range(T))\n",
      "sca2 -= sum(W[d,v] * sum(z[d,v,k] * safe_log(z[d,v,k]) for k in range(K)) \\\n",
      "           for v in range(T) for d in range(D))\n",
      "\n",
      "# Line 3 from Guillaume's note\n",
      "sca3 -= sum(n[d] * (\\\n",
      "            lxi[d,k]*(s[d]**2 - xi[d,k]**2) \\\n",
      "            - 0.5*(s[d] + xi[d,k]) \\\n",
      "            + log(1 + exp(xi[d,k])) \\\n",
      "            ) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 4 omitted from Guillaume's note\n",
      "# In my derivation it's -sum_d sum_n sum_k z_dnk s_d = -\\sum_d n_d s_d\n",
      "sca4 -= sum(n[d] * s[d] for d in range(D))\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (sca1, sca2, sca3, sca4, sca1+sca2+sca3+sca4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1058.621222 + -646.943452 + -6020.548604 + -981.298486 = -8707.411765\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now the sparse-optimized vectorized equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab64 = vocab.astype(np.float64)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec1, vec2, vec3, vec4 = 0, 0, 0, 0\n",
      "\n",
      "vec1 -= np.sum((M*M + V*V) * n[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * -0.5 * n[:,np.newaxis])\n",
      "vec1 += np.sum(M * gV)\n",
      "\n",
      "vec2 -= np.sum(M * gV) # Note the cancelation above\n",
      "vec2 -= -np.sum(sparseScalarProductOfSafeLnDot(Ws, gT, vocab64).data)\n",
      "\n",
      "vec3 -= np.sum(n[:,np.newaxis] * lxi * ((s*s)[:,np.newaxis] - (xi * xi)))\n",
      "vec3 += np.sum(0.5 * n[:,np.newaxis] * (s[:,np.newaxis] + xi))\n",
      "vec3 -= np.sum(n[:,np.newaxis] * np_safe_log(1 + np.exp(xi)))\n",
      "\n",
      "vec4 -= np.dot(s, n)\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (vec1, vec2, vec3, vec4, vec1+vec2+vec3+vec4))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1058.621222 + -646.943452 + -6020.548604 + -981.298486 = -8707.411765\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Determinant Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = np.array( [[0.4, 0, 0, 0], [0, 13.43, 0, 0], [0, 0, 0.12, 0], [0, 0, 0, 3.1459]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "array([[  0.4   ,   0.    ,   0.    ,   0.    ],\n",
        "       [  0.    ,  13.43  ,   0.    ,   0.    ],\n",
        "       [  0.    ,   0.    ,   0.12  ,   0.    ],\n",
        "       [  0.    ,   0.    ,   0.    ,   3.1459]])"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.det(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "2.027972976"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.product(np.diag(test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "2.027972976"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.log(la.det(test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "0.7070367601963653"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(np.log(np.diag(test)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "0.70703676019636541"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Update for the Topic Means"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "isigT = la.inv(sigT)\n",
      "\n",
      "d = 3\n",
      "v_d = 2 * s[d] * lxi[d,:] - 0.5 + 1./n[d] * gV[d,:]\n",
      "D_d = 2 * np.diag(lxi[d])\n",
      "lhs = la.inv(n[d] * D_d + isigT)\n",
      "rhs = n[d] * v_d + isigT.dot(mu)\n",
      "\n",
      "lhs.dot(rhs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vMat   = (2  * s[:,np.newaxis] * lxi - 0.5) * n[:,np.newaxis] + gV\n",
      "rhsMat = vMat + isigT.dot(mu)\n",
      "nD = n[d,np.newaxis] * 2 * lxi\n",
      "\n",
      "isigT\n",
      "means_d = la.inv(ssp.diags(nD[d,:], 0) + isigT).dot(rhsMat[d,:])\n",
      "means_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vMat   = (2  * s[:,np.newaxis] * lxi - 0.5) * n[:,np.newaxis] + gV\n",
      "rhsMat = vMat + isigT.dot(mu)\n",
      "\n",
      "isigT.flat[::K+1] += n[d] * 2 * lxi[d,:]\n",
      "means_d = la.inv(isigT).dot(rhsMat[d,:])\n",
      "isigT.flat[::K+1] -= n[d] * 2 * lxi[d,:]\n",
      "means_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rhsMat[d,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "array([-4.32225947, -3.80605519, -3.61484059, -3.76974105, -4.24516418,\n",
        "       -3.98314704])"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The complication is the left-hand side. It's a $K \\times K$ matrix for every document, and moreover, it's a matrix that needs to be _inverted_ for every document. Clearly, we really could do without this.\n",
      "\n",
      "What's truly awful though is that this requires that we use a `for` loop in Python over documents which is going to be dog slow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vMat   = (2  * s[:,np.newaxis] * lxi - 0.5) * n[:,np.newaxis] + gV\n",
      "rhsMat = vMat + isigT.dot(mu)\n",
      "nD = n[d,np.newaxis] * 2 * lxi\n",
      "\n",
      "isigT\n",
      "means_d = la.inv(ssp.diags(nD[d,:], 0) + isigT).dot(rhsMat[d,:])\n",
      "means_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lhs.dot(rhs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D_d = n[:,np.newaxis] * 2 * lxi[d,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.inv(ssp.diags(D_d[d,:], 0) + isigT).dot(rhs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rhs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "array([-4.32225947, -3.80605519, -3.61484059, -3.76974105, -4.24516418,\n",
        "       -3.98314704])"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rhsMat[d,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "array([-4.32225947, -3.80605519, -3.61484059, -3.76974105, -4.24516418,\n",
        "       -3.98314704])"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "isigT.flat[::K+1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "array([ 1.16033412,  1.64416707,  0.27369607,  1.78989091,  1.39072421,\n",
        "        1.46148196])"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testMat = np.arange(25).reshape((5,5))\n",
      "testMat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "array([[ 0,  1,  2,  3,  4],\n",
        "       [ 5,  6,  7,  8,  9],\n",
        "       [10, 11, 12, 13, 14],\n",
        "       [15, 16, 17, 18, 19],\n",
        "       [20, 21, 22, 23, 24]])"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testMat.flat[::6] -= 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testMat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "array([[-2,  1,  2,  3,  4],\n",
        "       [ 5,  4,  7,  8,  9],\n",
        "       [10, 11, 10, 13, 14],\n",
        "       [15, 16, 17, 16, 19],\n",
        "       [20, 21, 22, 23, 22]])"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Double Checking Guillaume's Optimisation\n",
      "\n",
      "The derivation is $(2 n_d \\text{diag}(\\Lambda(\\xi_d) + \\Sigma^{-1})^{-1}(n_d V_d + \\Sigma^{-1}\\mu)$. I've reproduced this with the conditional model.\n",
      "\n",
      "The derivation presented in algorithm 2 is $(2 n_d \\text{diag}(\\Lambda(\\xi_d))\\Sigma + I_K)^{-1}(n_d \\Sigma V_d + \\mu)$\n",
      "\n",
      "Note that algorithm 2 doesn't require us to invert $\\Sigma$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "isigT = la.inv(sigT)\n",
      "\n",
      "d = 3\n",
      "v_d = 2 * s[d] * lxi[d,:] - 0.5 + 1./n[d] * gV[d,:]\n",
      "D_d = 2 * np.diag(lxi[d])\n",
      "lhs = la.inv(n[d] * D_d + isigT)\n",
      "rhs = n[d] * v_d + isigT.dot(mu)\n",
      "\n",
      "original = lhs.dot(rhs)\n",
      "\n",
      "lhs = la.inv(n[d] * D_d * sigT + ssp.eye(K))\n",
      "rhs = n[d] * sigT.dot(v_d) + mu\n",
      "later = lhs.dot(rhs)\n",
      "\n",
      "[original, later]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "[array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812]),\n",
        " array([-5.41234938, -5.61626808, -4.01549496, -4.96089168, -5.84306868,\n",
        "       -5.47439266])]"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So in summary, the alternate model, which was evidentally trying to avoid inverting $\\Sigma$, just doesn't work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Double Checking the Vocabulary Update"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The scalar version"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = vocab.copy()\n",
      "\n",
      "for k in range(K):\n",
      "    denom = 0\n",
      "    for v in range(T):\n",
      "        for d in range(D):\n",
      "            denom += z[d,v,k] * W[d,v]\n",
      "    for v in range(T):\n",
      "        tot = 0\n",
      "        for d in range(D):\n",
      "            tot += z[d,v,k] * W[d,v]\n",
      "        sca[k,v] = tot / denom\n",
      "\n",
      "(sca * 100).astype(np.int32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "array([[34, 33, 32,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0, 33, 34, 32,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0, 33, 33, 32],\n",
        "       [34,  0,  0, 32,  0,  0, 33,  0,  0],\n",
        "       [ 0, 32,  0,  0, 34,  0,  0, 33,  0],\n",
        "       [ 0,  0, 33,  0,  0, 33,  0,  0, 33]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "vec = vocab.copy()\n",
      "\n",
      "vec *= (gR.T.dot(gT)).T # Awkward order to maintain sparsity (R is sparse, expMeans is dense)\n",
      "vec = normalize_rows_ip(vocab)\n",
      "vec += 1E-300\n",
      "\n",
      "(vec * 100).astype(np.int32)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "array([[33, 33, 33,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0, 33, 33, 33,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0, 33, 33, 33],\n",
        "       [33,  0,  0, 33,  0,  0, 33,  0,  0],\n",
        "       [ 0, 33,  0,  0, 33,  0,  0, 33,  0],\n",
        "       [ 0,  0, 33,  0,  0, 33,  0,  0, 33]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Test the update for the variances\n",
      "\n",
      "First we start with the scalar approach"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = np.ndarray(shape=(D,K))\n",
      "\n",
      "for d in range(D):\n",
      "    for k in range(K):\n",
      "        sca[d,k] = 1./ (2 * n[d] * lxi[d,k] + isigT[k,k])\n",
      "\n",
      "sca[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "array([[ 0.1648914 ,  0.15284357,  0.19154453,  0.14665464,  0.16226906,\n",
        "         0.15908661],\n",
        "       [ 0.19644031,  0.17801046,  0.23521601,  0.17744418,  0.19496482,\n",
        "         0.18927098],\n",
        "       [ 0.13644378,  0.13224514,  0.15860147,  0.12760714,  0.131087  ,\n",
        "         0.13230723],\n",
        "       [ 0.20915554,  0.18623569,  0.2539185 ,  0.1894067 ,  0.20116159,\n",
        "         0.20247671],\n",
        "       [ 0.20374469,  0.17930654,  0.23962508,  0.17385605,  0.18743828,\n",
        "         0.18633071],\n",
        "       [ 0.25624333,  0.23118107,  0.33138701,  0.2203449 ,  0.24647999,\n",
        "         0.2378013 ],\n",
        "       [ 0.169534  ,  0.15347017,  0.18961699,  0.15297388,  0.16499059,\n",
        "         0.15456316],\n",
        "       [ 0.20624939,  0.19399776,  0.25588357,  0.18291808,  0.19500744,\n",
        "         0.19291437],\n",
        "       [ 0.16853152,  0.15010197,  0.1962202 ,  0.14560341,  0.15605344,\n",
        "         0.15273506],\n",
        "       [ 0.1266008 ,  0.12034027,  0.15161288,  0.12008405,  0.12940583,\n",
        "         0.12345111]])"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = 1./(2 * n[:,np.newaxis] * lxi + isigT.flat[::K+1])\n",
      "\n",
      "vec[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "array([[ 0.1648914 ,  0.15284357,  0.19154453,  0.14665464,  0.16226906,\n",
        "         0.15908661],\n",
        "       [ 0.19644031,  0.17801046,  0.23521601,  0.17744418,  0.19496482,\n",
        "         0.18927098],\n",
        "       [ 0.13644378,  0.13224514,  0.15860147,  0.12760714,  0.131087  ,\n",
        "         0.13230723],\n",
        "       [ 0.20915554,  0.18623569,  0.2539185 ,  0.1894067 ,  0.20116159,\n",
        "         0.20247671],\n",
        "       [ 0.20374469,  0.17930654,  0.23962508,  0.17385605,  0.18743828,\n",
        "         0.18633071],\n",
        "       [ 0.25624333,  0.23118107,  0.33138701,  0.2203449 ,  0.24647999,\n",
        "         0.2378013 ],\n",
        "       [ 0.169534  ,  0.15347017,  0.18961699,  0.15297388,  0.16499059,\n",
        "         0.15456316],\n",
        "       [ 0.20624939,  0.19399776,  0.25588357,  0.18291808,  0.19500744,\n",
        "         0.19291437],\n",
        "       [ 0.16853152,  0.15010197,  0.1962202 ,  0.14560341,  0.15605344,\n",
        "         0.15273506],\n",
        "       [ 0.1266008 ,  0.12034027,  0.15161288,  0.12008405,  0.12940583,\n",
        "         0.12345111]])"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 86,
       "text": [
        "array([[1, 2, 3],\n",
        "       [4, 5, 6],\n",
        "       [7, 8, 9]])"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A.flat[::4] += 4\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "array([[ 5,  2,  3],\n",
        "       [ 4,  9,  6],\n",
        "       [ 7,  8, 13]])"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Verifying the update for the topic covariance matrix\n",
      "\n",
      "The derivation of this is pretty straightforward: it's the sample covariance of the means, plus the sum of their individual, diagonal variances $V_d$. So\n",
      "\n",
      "$\\Sigma = \\frac{1}{D}\\sum_d \\left[ (m_d - \\mu)(m_d - \\mu)^{\\top} + V_d \\right]$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = np.ndarray(shape=(K,K))\n",
      "sca.fill(0)\n",
      "\n",
      "for d in range(D):\n",
      "    dif = M[d,K] - mu\n",
      "    sca += dif.dot(dif.T)\n",
      "    sca += "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "Required argument 'object' (pos 1) not found",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-91-d855753685c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = np.array([1, 2, 3, 4, 5])\n",
      "A = np.ones((5,5))\n",
      "\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([[ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.]])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.dot(np.eye(5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([ 1.,  2.,  3.,  4.,  5.])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.dot(np.ones((5,5)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([ 15.,  15.,  15.,  15.,  15.])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.T.dot(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "55"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.T.dot(np.ones((5,)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "15.0"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.norm(z,1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'norm'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-71346154b836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'norm'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.norm(z,1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "15"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "array([1, 2, 3, 4, 5])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.dot(np.diag(1./z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([ 1.,  1.,  1.,  1.,  1.])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([[ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.]])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.eye(5) - 0.2\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "array([[ 0.8, -0.2, -0.2, -0.2, -0.2],\n",
        "       [-0.2,  0.8, -0.2, -0.2, -0.2],\n",
        "       [-0.2, -0.2,  0.8, -0.2, -0.2],\n",
        "       [-0.2, -0.2, -0.2,  0.8, -0.2],\n",
        "       [-0.2, -0.2, -0.2, -0.2,  0.8]])"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A.dot(np.ones((5,)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([  5.55111512e-17,   5.55111512e-17,   0.00000000e+00,\n",
        "        -5.55111512e-17,   0.00000000e+00])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.dot(np.diag(1./z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array([ 1.,  1.,  1.,  1.,  1.])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "array([1, 2, 3, 4, 5])"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.linspace(1, 5*5, 5*5).reshape(5,5)\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "array([[  1.,   2.,   3.,   4.,   5.],\n",
        "       [  6.,   7.,   8.,   9.,  10.],\n",
        "       [ 11.,  12.,  13.,  14.,  15.],\n",
        "       [ 16.,  17.,  18.,  19.,  20.],\n",
        "       [ 21.,  22.,  23.,  24.,  25.]])"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.T.dot(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([ 215.,  230.,  245.,  260.,  275.])"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(z[j] * A[j,3] for j in range(5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "260.0"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.eye(5) - 0.2\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array([[ 0.8, -0.2, -0.2, -0.2, -0.2],\n",
        "       [-0.2,  0.8, -0.2, -0.2, -0.2],\n",
        "       [-0.2, -0.2,  0.8, -0.2, -0.2],\n",
        "       [-0.2, -0.2, -0.2,  0.8, -0.2],\n",
        "       [-0.2, -0.2, -0.2, -0.2,  0.8]])"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.dot(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "array([ -2.00000000e+00,  -1.00000000e+00,   2.22044605e-16,\n",
        "         1.00000000e+00,   2.00000000e+00])"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z.T.dot(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "array([ -2.00000000e+00,  -1.00000000e+00,   2.22044605e-16,\n",
        "         1.00000000e+00,   2.00000000e+00])"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A.dot(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "array([ -2.00000000e+00,  -1.00000000e+00,   2.22044605e-16,\n",
        "         1.00000000e+00,   2.00000000e+00])"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A.T.dot(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "array([ -2.00000000e+00,  -1.00000000e+00,   2.22044605e-16,\n",
        "         1.00000000e+00,   2.00000000e+00])"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}