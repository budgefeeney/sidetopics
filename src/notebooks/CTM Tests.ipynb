{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.random as rd\n",
      "import scipy as sp\n",
      "import scipy.sparse as ssp\n",
      "import scipy.linalg as la\n",
      "import scipy.sparse.linalg as sla\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "\n",
      "from math import log, pi, exp\n",
      "\n",
      "rd.seed(0xC0FFEE)\n",
      "\n",
      "%load_ext cythonmagic\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The cythonmagic extension is already loaded. To reload it, use:\n",
        "  %reload_ext cythonmagic\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize_rows_ip(matrix):\n",
      "    '''\n",
      "    Normalizes the rows of the matrix in-place, ovewriting\n",
      "    the previous values\n",
      "    '''\n",
      "    row_sums  = matrix.sum(axis=1)\n",
      "    matrix   /= row_sums[:, np.newaxis]\n",
      "    return matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Artifical Dataset\n",
      "## Vocabularies\n",
      "We create six hard-coded vocabularies, each of which can easily be visualized as $3 \\times 3$ square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T = 3 * 3\n",
      "K = 6\n",
      "\n",
      "# Horizontal bars\n",
      "vocab1 = ssp.coo_matrix(([1, 1, 1], ([0, 0, 0], [0, 1, 2])), shape=(3,3)).todense()\n",
      "vocab2 = ssp.coo_matrix(([1, 1, 1], ([1, 1, 1], [0, 1, 2])), shape=(3,3)).todense()\n",
      "vocab3 = ssp.coo_matrix(([1, 1, 1], ([2, 2, 2], [0, 1, 2])), shape=(3,3)).todense()\n",
      "\n",
      "# Vertical bars\n",
      "vocab4 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [0, 0, 0])), shape=(3,3)).todense()\n",
      "vocab5 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [1, 1, 1])), shape=(3,3)).todense()\n",
      "vocab6 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [2, 2, 2])), shape=(3,3)).todense()\n",
      "\n",
      "# Diagonals\n",
      "vocab7 = ssp.coo_matrix(([1, 1, 1], ([0, 1, 2], [0, 1, 2])), shape=(3,3)).todense()\n",
      "vocab8 = ssp.coo_matrix(([1, 1, 1], ([2, 1, 0], [0, 1, 2])), shape=(3,3)).todense()\n",
      "\n",
      "# Put together\n",
      "T = vocab1.shape[0] * vocab1.shape[1]\n",
      "vocabs = [vocab1, vocab2, vocab3, vocab4, vocab5, vocab6] #, vocab7, vocab8]\n",
      "\n",
      "# Create a single matrix with the flattened vocabularies\n",
      "vocabVectors = []\n",
      "for vocab in vocabs:\n",
      "    vocabVectors.append (np.squeeze(np.asarray (vocab.reshape((1,T)))))\n",
      "\n",
      "vocab = normalize_rows_ip(np.array(vocabVectors, dtype=np.float32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ones = np.ones(vocabs[0].shape)\n",
      "for k in range(K):\n",
      "    plt.subplot(2, 3, k)\n",
      "    plt.imshow(ones - vocabs[k], interpolation=\"none\", cmap = cm.Greys_r)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD9CAYAAAC7iRw+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10be74b50>"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Topic Distributions and the Document-Term Matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rd.seed(0xBADB055)\n",
      "D = 100\n",
      "\n",
      "topicDist = normalize_rows_ip (rd.random((D,K)))\n",
      "\n",
      "meanWordCount = 20\n",
      "wordCounts = rd.poisson(meanWordCount, size=D)\n",
      "W = np.floor(topicDist.dot(vocab) * wordCounts[:, np.newaxis])\n",
      "\n",
      "Ws = ssp.csr_matrix(W)\n",
      "print (\"Non-zero count is \\n\" + str(Ws.nonzero))\n",
      "\n",
      "print (\"\\n\\n Matrix\" + str(W[0:10,:]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Non-zero count is \n",
        "<bound method csr_matrix.nonzero of <100x9 sparse matrix of type '<class 'numpy.float64'>'\n",
        "\twith 830 stored elements in Compressed Sparse Row format>>\n",
        "\n",
        "\n",
        " Matrix[[ 2.  1.  3.  1.  1.  2.  1.  1.  2.]\n",
        " [ 0.  1.  2.  1.  1.  2.  4.  4.  6.]\n",
        " [ 3.  4.  5.  1.  2.  3.  1.  1.  3.]\n",
        " [ 1.  1.  3.  3.  2.  4.  1.  0.  3.]\n",
        " [ 2.  2.  2.  2.  1.  1.  1.  1.  1.]\n",
        " [ 2.  1.  2.  1.  1.  1.  3.  2.  3.]\n",
        " [ 0.  2.  3.  3.  4.  6.  1.  2.  4.]\n",
        " [ 1.  2.  1.  1.  2.  1.  1.  3.  2.]\n",
        " [ 1.  1.  0.  2.  2.  1.  2.  3.  2.]\n",
        " [ 1.  1.  2.  3.  2.  4.  3.  2.  3.]]\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the full list of topic distributions for all $D$ documents presented in percentage form."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(topicDist * 100).astype(np.int32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "array([[20, 10,  9, 20, 10, 29],\n",
        "       [ 1, 11, 48,  3, 11, 24],\n",
        "       [29, 10,  5, 10, 16, 28],\n",
        "       [15, 36, 10,  5,  1, 31],\n",
        "       [31, 17, 11, 15, 11, 12],\n",
        "       [18,  9, 32, 17,  4, 16],\n",
        "       [ 0, 23,  5,  7, 24, 38],\n",
        "       [16, 12, 22,  7, 27, 13],\n",
        "       [ 8, 17, 31, 16, 20,  4],\n",
        "       [ 2, 24, 20, 17,  9, 24],\n",
        "       [21,  4, 25,  4, 26, 17],\n",
        "       [ 1, 19,  1, 28, 27, 21],\n",
        "       [31,  8, 25, 11, 22,  1],\n",
        "       [ 5, 23,  6, 24, 16, 23],\n",
        "       [ 1, 14, 26, 31,  2, 22],\n",
        "       [ 6,  2, 27,  6, 23, 34],\n",
        "       [17, 18, 21,  4, 15, 21],\n",
        "       [23, 22,  3,  3, 22, 23],\n",
        "       [30, 24, 25,  3,  6,  9],\n",
        "       [20, 23, 11,  9, 11, 24],\n",
        "       [19, 32, 28,  0, 12,  6],\n",
        "       [20, 15, 38, 17,  5,  3],\n",
        "       [19, 24, 33,  9,  6,  7],\n",
        "       [24, 14, 15, 16,  9, 20],\n",
        "       [ 4, 18,  7, 30, 21, 17],\n",
        "       [18, 12, 31, 16, 17,  2],\n",
        "       [22, 26,  5,  8, 22, 14],\n",
        "       [11, 12, 16, 20, 27, 12],\n",
        "       [26, 10,  7, 11, 16, 28],\n",
        "       [24,  9, 22, 12, 14, 15],\n",
        "       [25, 32, 10,  3, 14, 14],\n",
        "       [32,  2, 27, 16, 17,  3],\n",
        "       [16,  7, 24, 24, 16, 11],\n",
        "       [ 8, 25, 19, 23, 18,  4],\n",
        "       [ 8, 25, 10, 39,  3, 11],\n",
        "       [23, 14,  6, 29, 14, 11],\n",
        "       [ 3,  9, 18, 16, 30, 22],\n",
        "       [ 9,  0, 37,  0, 32, 18],\n",
        "       [31, 16,  9,  0, 13, 27],\n",
        "       [19, 32,  7,  7,  3, 29],\n",
        "       [ 6, 21,  3,  7, 32, 27],\n",
        "       [14, 26, 22,  9,  5, 20],\n",
        "       [14, 26, 24,  4, 23,  6],\n",
        "       [18, 25, 10,  1, 23, 21],\n",
        "       [11, 31, 31,  6, 12,  6],\n",
        "       [18, 18, 10, 10, 22, 19],\n",
        "       [22, 24, 15, 22,  4, 10],\n",
        "       [20, 14, 18, 12, 18, 16],\n",
        "       [13, 32,  9,  4, 23, 16],\n",
        "       [33, 10,  7, 10, 17, 20],\n",
        "       [ 6, 22, 21,  8, 20, 20],\n",
        "       [27, 10, 20,  4, 19, 17],\n",
        "       [19, 14, 37, 14,  1, 13],\n",
        "       [24, 15, 19, 20,  7, 13],\n",
        "       [10, 14, 19, 21, 19, 14],\n",
        "       [21, 18, 13, 10, 12, 23],\n",
        "       [10,  2, 20,  7, 40, 18],\n",
        "       [ 8, 14, 12, 28, 14, 21],\n",
        "       [ 0, 19, 24, 19, 17, 18],\n",
        "       [14, 11, 13, 27, 16, 16],\n",
        "       [21,  0, 11, 20, 19, 26],\n",
        "       [12, 17, 49,  0,  2, 16],\n",
        "       [ 9,  9, 20, 30, 22,  7],\n",
        "       [ 5, 33,  4, 12, 31, 13],\n",
        "       [15, 14, 11, 31, 12, 14],\n",
        "       [16, 36, 15,  3, 13, 14],\n",
        "       [11, 28, 28, 10,  6, 14],\n",
        "       [17, 11, 20,  3, 10, 36],\n",
        "       [21, 23, 15,  0,  8, 31],\n",
        "       [23,  1,  4, 31, 32,  6],\n",
        "       [20, 11,  4, 31, 10, 22],\n",
        "       [22, 12, 18, 14, 16, 15],\n",
        "       [19, 28,  5, 13, 24,  7],\n",
        "       [29, 15,  3, 24, 20,  7],\n",
        "       [11,  4, 41, 15,  2, 25],\n",
        "       [ 8, 11, 31,  8, 23, 15],\n",
        "       [30, 14,  8, 15,  0, 31],\n",
        "       [19, 21, 25, 10, 15,  6],\n",
        "       [10,  8, 14, 28, 20, 18],\n",
        "       [13, 22,  5, 23, 20, 13],\n",
        "       [37, 23,  4, 21,  9,  4],\n",
        "       [28, 14,  0,  5, 27, 23],\n",
        "       [27, 27,  8, 11, 17,  5],\n",
        "       [32, 21, 12, 13,  7, 12],\n",
        "       [14,  7, 21, 36,  8, 12],\n",
        "       [ 2, 23, 31, 24,  2, 16],\n",
        "       [27, 17, 20,  8, 14, 11],\n",
        "       [15, 26,  9, 11, 30,  7],\n",
        "       [35, 28, 20,  4,  4,  6],\n",
        "       [ 0,  7,  4, 48, 26, 11],\n",
        "       [ 1, 21, 28, 27, 10,  9],\n",
        "       [24, 27,  9,  5, 15, 17],\n",
        "       [31, 26, 20,  6,  9,  4],\n",
        "       [39,  8, 12,  6, 18, 14],\n",
        "       [27,  7,  8, 32, 19,  3],\n",
        "       [ 2, 10, 15, 27, 19, 25],\n",
        "       [25, 25,  2, 17, 16, 12],\n",
        "       [ 7, 43,  6, 24,  3, 16],\n",
        "       [12, 26,  5, 33, 20,  1],\n",
        "       [ 5,  8, 22, 18, 21, 21]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(vocab * 100).astype(np.int32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 84,
       "text": [
        "array([[33, 33, 33,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0, 33, 33, 33,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0, 33, 33, 33],\n",
        "       [33,  0,  0, 33,  0,  0, 33,  0,  0],\n",
        "       [ 0, 33,  0,  0, 33,  0,  0, 33,  0],\n",
        "       [ 0,  0, 33,  0,  0, 33,  0,  0, 33]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordCounts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "array([18, 25, 27, 22, 18, 22, 29, 20, 18, 25, 24, 20, 21, 25, 22, 10, 25,\n",
        "       15,  9, 18, 16, 23, 21, 18, 25, 17, 18, 18, 16,  9, 20, 16, 23, 21,\n",
        "       17, 20, 23, 16, 13, 26, 17, 18, 17, 18, 23, 23, 19, 18, 19, 16, 19,\n",
        "       14, 21, 24, 18, 25, 26, 15, 21, 14, 20, 27, 21, 20, 21, 20, 26, 11,\n",
        "       27, 20, 22, 18, 18, 22, 13, 17, 25, 17, 21, 22, 25, 20, 17, 22, 12,\n",
        "       26, 25, 20, 21, 18, 24, 18, 21, 22, 16, 19, 22, 21, 16, 26])"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Proof of Vectorizations\n",
      "\n",
      "## Bound on the topic means\n",
      "\n",
      "$\\sum_d \\left[  -\\frac{K}{2} \\ln 2\\pi - \\frac{1}{2}\\ln|\\Sigma| - \\frac{1}{2} \\left( (m_d - \\mu)^{\\top} \\Sigma^{-1} (m_d - \\mu) \\right) + \\text{tr}(V_d \\Sigma^{-1})\\right]$\n",
      "\n",
      "where $V_d$ is the diagaonal variance associated with the posterior mean."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu   = rd.random((K,))\n",
      "mu  /= np.sum(mu)\n",
      "sigT = rd.random((K,K)) * 2\n",
      "sigT = sigT.T.dot(sigT)\n",
      "M    = normalize_rows_ip(rd.random((D,K)))\n",
      "V    = rd.random((D,K))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Scalar\n",
      "sca = 0\n",
      "for d in range(D):\n",
      "    sca -= K/2. * np.log(2*pi)\n",
      "    sca -= 1/2. * la.det(sigT)\n",
      "    sca -= 1/2. * (M[d,:] - mu).T.dot(la.inv(sigT)).dot(M[d,:] - mu)\n",
      "    sca -= 1/2. * np.trace(np.diag(V[d,:]).dot(la.inv(sigT)))\n",
      "\n",
      "print(\"%f\" % sca)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1289.554994\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Vectorization\n",
      "vec = 0\n",
      "\n",
      "vec -= (D*K)/2. * np.log(2*pi)\n",
      "vec -= D/2. * la.det(sigT)\n",
      "dif = M - mu[np.newaxis,:]\n",
      "vec -= 0.5 * (np.sum (dif.dot(la.inv(sigT)) * dif))\n",
      "vec -= 0.5 * np.sum(V * np.diag(la.inv(sigT))[np.newaxis,:]) # = sum_d tr(V_d \\Sigma^{-1})\n",
      "             \n",
      "print(\"%f\" % vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1289.554994\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bound on Approx to Z_dvk\n",
      "\n",
      "This is a bit of a horror show, as it takes into account several variables in the approximation.\n",
      "\n",
      "First we define a couple of useful functions, and create all the variables that we need. This includes the memberships $Z \\in \\mathbb{R}^{D \\times T \\ \\times K}$. \n",
      "\n",
      "In the vectorized form, $Z$ no longer exists, as we've subsituted in the solution for $Z$ directly into all the equations, so we don't have to materialize such an enormous tensor "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LOG_ZERO = log(1E-300)\n",
      "\n",
      "def safe_log(x):\n",
      "    return log(max(x, 1E-300))\n",
      "\n",
      "def np_safe_log(X):\n",
      "    out = np.ndarray(X.shape)\n",
      "    out.fill (LOG_ZERO)\n",
      "    out[X > 1E-300] = np.log(X[X > 1E-200])\n",
      "    return out\n",
      "    \n",
      "\n",
      "def negJakkola(vec):\n",
      "    '''\n",
      "    The negated version of the Jakkola expression which was used in Bouchard's NIPS\n",
      "    2007 softmax bound\n",
      "    \n",
      "    CTM Source reads: y = .5./x.*(1./(1+exp(-x)) -.5);\n",
      "    '''\n",
      "    \n",
      "    #\u00a0COPY AND PASTE BETWEEN THIS AND negJakkolaOfDerivedXi()\n",
      "    return 0.5/vec * (1./(1 + np.exp(-vec)) - 0.5)\n",
      "\n",
      "def deriveXi (means, varcs, s):\n",
      "    '''\n",
      "    Derives a value for xi. This is not normally needed directly, as we\n",
      "    normally just work with the negJakkola() function of it\n",
      "    '''\n",
      "    return np.sqrt(means**2 - 2 * means * s[:,np.newaxis] + (s**2)[:,np.newaxis] + varcs**2)   \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s   = rd.random((D,))\n",
      "xi  = deriveXi(M, V, s)\n",
      "lxi = negJakkola(xi)\n",
      "eM  = np.exp(M)\n",
      "\n",
      "z = np.ndarray(shape=(D,T,K))\n",
      "z.fill(0)\n",
      "\n",
      "for d in range(D):\n",
      "    for v in range(T):\n",
      "        denom = 0\n",
      "        for j in range(K):\n",
      "            denom += vocab[j,v] * eM[d,j]\n",
      "        for k in range(K):\n",
      "            z[d,v,k] = vocab[k,v] * eM[d,k] / denom\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gT = np.exp(M)     # D x T\n",
      "gU = gT.dot(vocab) # D x V\n",
      "gR = W / gU        # D x V\n",
      "gV = gT * (gR.dot(vocab.T)) # D x K\n",
      "\n",
      "vocabLen = vocab.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[sum(W[0,v] * z[0,v,k] for v in range(T)) for k in range(K)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "[2.8167487786653025,\n",
        " 1.8903623759529875,\n",
        " 1.8149695259722982,\n",
        " 2.1334482622379252,\n",
        " 1.5795334053986938,\n",
        " 3.7649376517727928]"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gV[0,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "array([ 2.81674878,  1.89036238,  1.81496953,  2.13344826,  1.57953341,\n",
        "        3.76493765])"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the functions defined and sample values listed, we now compare the scalar and vectorized forms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = wordCounts\n",
      "\n",
      "sca1, sca2, sca3, sca4 = 0, 0, 0, 0\n",
      "\n",
      "# Line 1 from Guillaume's note\n",
      "sca1 -= sum((M[d,k]**2 + V[d,k]**2) * n[d] * lxi[d,k] \\\n",
      "           for k in range(K) for d in range(D))\n",
      "sca1 += sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d] \\\n",
      "            + sum(W[d,v] * z[d,v,k] for v in range(T))) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 2 from Guillaume's note\n",
      "sca2 += sum(safe_log(vocab[k,v]) * (sum (W[d,v] * z[d,v,k] for d in range(D))) \\\n",
      "            for k in range(K) for v in range(T))\n",
      "sca2 -= sum(W[d,v] * sum(z[d,v,k] * safe_log(z[d,v,k]) for k in range(K)) \\\n",
      "           for v in range(T) for d in range(D))\n",
      "\n",
      "# Line 3 from Guillaume's note\n",
      "sca3 -= sum(n[d] * (\\\n",
      "            lxi[d,k]*(s[d]**2 - xi[d,k]**2) \\\n",
      "            - 0.5*(s[d] + xi[d,k]) \\\n",
      "            + log(1 + exp(xi[d,k])) \\\n",
      "            ) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 4 omitted from Guillaume's note\n",
      "# In my derivation it's -sum_d sum_n sum_k z_dnk s_d = -\\sum_d n_d s_d\n",
      "sca4 -= sum(n[d] * s[d] for d in range(D))\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (sca1, sca2, sca3, sca4, sca1+sca2+sca3+sca4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-988.431067 + -627.348414 + -5560.977364 + -1097.675658 = -8274.432504\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A Quick test of sums over W and Z"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = 0\n",
      "for d in range(D):\n",
      "    for k in range(K):\n",
      "        for v in range(vocabLen):\n",
      "            sca += W[d,v] * z[d,v,k]\n",
      "\n",
      "vec = np.sum(M.T.dot(gV))\n",
      "\n",
      "print (\"Scalar = %f, Vectorized = %f, W.sum=%f\\n\" %(sca, vec, W.sum()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Scalar = 1536.000000, Vectorized = 1536.000000, W.sum=1536.000000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, once you think about it, it's a weighted sum of the per-topic word-counts per documents, weighted by the topic assignments per document, which is basically the sum of the word-counts over all documents\n",
      "\n",
      "Finally the vectorized form of the bound"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec1, vec2, vec3, vec4 = 0, 0, 0, 0\n",
      "\n",
      "vec1 -= np.sum((M*M + V*V) * n[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * -0.5 * n[:,np.newaxis])\n",
      "vec1 += np.sum(M * gV)\n",
      "\n",
      "vec2 -= np.sum(M * gV) # Note the cancelation above\n",
      "vec2 -= -np.sum(np_safe_log(gU) * W) # R * TB = W\n",
      "\n",
      "vec3 -= np.sum(n[:,np.newaxis] * lxi * ((s*s)[:,np.newaxis] - (xi * xi)))\n",
      "vec3 += np.sum(0.5 * n[:,np.newaxis] * (s[:,np.newaxis] + xi))\n",
      "vec3 -= np.sum(n[:,np.newaxis] * np_safe_log(1 + np.exp(xi)))\n",
      "\n",
      "vec4 -= np.dot(s, n)\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (vec1, vec2, vec3, vec4, vec1+vec2+vec3+vec4))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-988.431067 + -627.348414 + -5560.977364 + -1097.675658 = -8274.432504\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second line involves the expected log-likely of p(W) and the entropy of Z"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = np.sum(np_safe_log(gU) * W)\n",
      "sca = sum(safe_log(vocab[k,v]) * (sum (W[d,v] * z[d,v,k] for d in range(D))) for k in range(K) for v in range(T))\n",
      "\n",
      "sca,vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 97,
       "text": [
        "(-1687.4684296178502, -361.5931084751798)"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Breakdown of Line 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum((M*M + V*V) * n[:,np.newaxis] * lxi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "518.98593063581541"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum((M[d,k]**2 + V[d,k]**2) * n[d] * lxi[d,k] \\\n",
      "           for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 99,
       "text": [
        "518.98593063581541"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi) \\\n",
      "+ np.sum(M * -0.5 * n[:,np.newaxis]) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "-735.20044160559326"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d]) for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 101,
       "text": [
        "-735.20044160559371"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(M * gV)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "265.75530518735991"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(M[d,k] * sum(W[d,v] * z[d,v,k] for v in range(T)) \\\n",
      "            for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 103,
       "text": [
        "265.75530518735985"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi) \\\n",
      "+ np.sum(M * -0.5 * n[:,np.newaxis]) \\\n",
      "+ np.sum(M * gV)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "-469.44513641823335"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d] \\\n",
      "              +sum(W[d,v] * z[d,v,k] for v in range(T))) \\\n",
      "              for k in range(K) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 105,
       "text": [
        "-469.4451364182338"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Breakdown of Line 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca21 = sum(safe_log(vocab[k,v]) * (sum (W[d,v] * z[d,v,k] for d in range(D))) for k in range(K) for v in range(T))\n",
      "sca22 = -sum(W[d,v] * sum(z[d,v,k] * safe_log(z[d,v,k]) for k in range(K)) \\\n",
      "           for v in range(T) for d in range(D))\n",
      "\n",
      "sca21, sca22, sca21 + sca22"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 106,
       "text": [
        "(-1687.4684296178502, 1060.1200159553111, -627.34841366253909)"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(np_safe_log(vocab) * vocab * gT.T.dot(gR))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 107,
       "text": [
        "-1687.4684296178505"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v1 = -np.sum(M * gV)\n",
      "v2 = np.sum(np_safe_log(gU) * W)\n",
      "\n",
      "v1, v2, v1+v2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 108,
       "text": [
        "(-265.75530518735991, -361.5931084751798, -627.34841366253977)"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec21 = np.sum(np_safe_log(vocab) * vocab * gT.T.dot(gR))\n",
      "vec22 = np.sum(M * gV)\n",
      "vec23 = -np.sum(np_safe_log(gU) * gR * gT.dot(vocab))\n",
      "\n",
      "print (\"%f + %f + %f = %f\\n\" % (vec21, vec22, vec23, vec21 + vec22 + vec23))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1687.468430 + 265.755305 + 361.593108 = -1060.120016\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bounds on Z_dvk Using Sparse Computations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##\u00a0Cython Helpers\n",
      "The matrix $U$ is a _dense_ $D \\times T$ matrix. However it is always used in conjunction with the _sparse_ $D \\times T$ document-term matrix $W$. Consequently, there is a massive performance improvement to be gained by only calculating the values of $U$ for the non-zero elements of $W$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython -c=-msse -c=-msse2 -c=-msse3 -c=-march=native -c=-mfpmath=sse\n",
      "\n",
      "cimport cython\n",
      "import numpy as np\n",
      "cimport numpy as np\n",
      "from libc.math cimport log\n",
      "from libc.float cimport FLT_MIN, DBL_MIN\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def sparseScalarQuotientOfDot_f8(double[:] A_data, int[:] A_indices, int[:] A_ptr, double[:,:] B, double[:,:] C, double[:] out_data):\n",
      "    '''\n",
      "    Returns A / np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A_data    - the values buffer of the sparse CSR matrix A\n",
      "    A_indices - the indices buffer of the sparse CSR matrix A\n",
      "    A_ptr     - the index pointer buffer of the sparse CSR matrix A\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out_data  - the values buffer into which the result will be placed.\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    cdef int rowCount = len(A_ptr) - 1 \n",
      "    cdef int elemCount = 0, e = 0\n",
      "    cdef int row = 0, col = 0, i = 0\n",
      "    with nogil:\n",
      "        while row < rowCount:\n",
      "            elemCount = A_ptr[row+1] - A_ptr[row]\n",
      "            e = 0\n",
      "            while e < elemCount:\n",
      "                col = A_indices[i]\n",
      "                out_data[i] = A_data[i] / dotProduct_f8(row,col,B,C)\n",
      "                i += 1\n",
      "                e += 1\n",
      "            row += 1\n",
      "    \n",
      "    return out_data\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def sparseScalarProductOfDot_f8(double[:] A_data, int[:] A_indices, int[:] A_ptr, double[:,:] B, double[:,:] C, double[:] out_data):\n",
      "    '''\n",
      "    Returns A * np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A_data    - the values buffer of the sparse CSR matrix A\n",
      "    A_indices - the indices buffer of the sparse CSR matrix A\n",
      "    A_ptr     - the index pointer buffer of the sparse CSR matrix A\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out_data  - the values buffer into which the result will be placed.\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    cdef int rowCount = len(A_ptr) - 1 \n",
      "    cdef int elemCount = 0, e = 0\n",
      "    cdef int row = 0, col = 0, i = 0\n",
      "    with nogil:\n",
      "        while row < rowCount:\n",
      "            elemCount = A_ptr[row+1] - A_ptr[row]\n",
      "            e = 0\n",
      "            while e < elemCount:\n",
      "                col = A_indices[i]\n",
      "                out_data[i] = A_data[i] * dotProduct_f8(row,col,B,C)\n",
      "                i += 1\n",
      "                e += 1\n",
      "            row += 1\n",
      "    \n",
      "    return out_data\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def sparseScalarProductOfSafeLnDot_f8(double[:] A_data, int[:] A_indices, int[:] A_ptr, double[:,:] B, double[:,:] C, double[:] out_data):\n",
      "    '''\n",
      "    Returns A * np.log(np.dot(B, C)), however it does so keeping in \n",
      "    mind the sparsity of A, calculate values only when required.\n",
      "    Moreover if any product of the dot is zero, it's replaced with\n",
      "    the minimum non-zero value allowed by the datatype, to avoid NaNs\n",
      "     \n",
      "    Params\n",
      "    A_data    - the values buffer of the sparse CSR matrix A\n",
      "    A_indices - the indices buffer of the sparse CSR matrix A\n",
      "    A_ptr     - the index pointer buffer of the sparse CSR matrix A\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out_data  - the values buffer into which the result will be placed.\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    cdef int rowCount = len(A_ptr) - 1 \n",
      "    cdef int elemCount = 0, e = 0\n",
      "    cdef int row = 0, col = 0, i = 0\n",
      "    cdef double dotProd = 0.0\n",
      "    cdef double logOfMin = log (DBL_MIN)\n",
      "    \n",
      "    with nogil:\n",
      "        while row < rowCount:\n",
      "            elemCount = A_ptr[row+1] - A_ptr[row]\n",
      "            e = 0\n",
      "            while e < elemCount:\n",
      "                col         = A_indices[i]\n",
      "                dotProd     = dotProduct_f8(row,col,B,C)\n",
      "                out_data[i] = A_data[i] * log(dotProd) if dotProd > DBL_MIN else logOfMin\n",
      "                i += 1\n",
      "                e += 1\n",
      "            row += 1\n",
      "    \n",
      "    return out_data\n",
      "\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "cdef double dotProduct_f8 (int r, int c, double[:,:] B, double[:,:] C) nogil:\n",
      "    '''\n",
      "    The dot product of the r-th row of B and the c-th column of C.\n",
      "    Done directly with a for-loop, no BLAS, SSE or anything. Still\n",
      "    pretty fast though - just as quick as a numpy dot\n",
      "    '''\n",
      "\n",
      "    cdef double result = 0\n",
      "    cdef int innerDim = B.shape[1]\n",
      "    \n",
      "    cdef int i = 0\n",
      "    while i < innerDim:\n",
      "        result += B[r,i] * C[i,c]\n",
      "        i += 1\n",
      "\n",
      "    return result\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import autojit\n",
      "\n",
      "def sparseScalarQuotientOfDot (A, B, C, out=None):\n",
      "    '''\n",
      "    Returns A / np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A         - a sparse CSR matrix\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out       - if specified, must be a sparse CSR matrix with identical\n",
      "                non-zero pattern to A (i.e. same indices and indptr)\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "        \n",
      "    if A.dtype == np.float64:\n",
      "        sparseScalarQuotientOfDot_f8(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    elif A.dtype == np.float32:\n",
      "        sparseScalarQuotientOfDot_f4(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    else:\n",
      "        _sparseScalarQuotientOfDot_py(A,B,C, out)\n",
      "    return out\n",
      "\n",
      "\n",
      "def sparseScalarProductOfDot (A, B, C, out=None):\n",
      "    '''\n",
      "    Returns A * np.dot(B, C), however it does so keeping in  mind \n",
      "    the sparsity of A, calculating values only where required.\n",
      "     \n",
      "    Params\n",
      "    A         - a sparse CSR matrix\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out       - if specified, must be a sparse CSR matrix with identical\n",
      "                non-zero pattern to A (i.e. same indices and indptr)\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if A.dtype == np.float64:\n",
      "        sparseScalarQuotientOfDot_f8(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    elif A.dtype == np.float32:\n",
      "        sparseScalarQuotientOfDot_f4(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    else:\n",
      "        _sparseScalarQuotientOfDot_py(A,B,C, out)\n",
      "    return out\n",
      "\n",
      "def _sparseScalarProductOfDot_py(A,B,C, out=None):\n",
      "    '''\n",
      "    Calculates A * B.dot(C) where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based multiplication.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if out is not A:\n",
      "        out.data[:] = A.data\n",
      "    \n",
      "    out.data *= B.dot(C)[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def sparseScalarProductOfSafeLnDot (A, B, C, out=None):\n",
      "    '''\n",
      "    Returns A * np.log(np.dot(B, C)), however it does so keeping in\n",
      "    mind the sparsity of A, calculating values only where required.\n",
      "    Moreover if any product of the dot is zero, it's replaced with\n",
      "    the minimum non-zero value allowed by the datatype, to avoid NaNs\n",
      "     \n",
      "    Params\n",
      "    A         - a sparse CSR matrix\n",
      "    B         - a dense matrix\n",
      "    C         - a dense matrix\n",
      "    out       - if specified, must be a sparse CSR matrix with identical\n",
      "                non-zero pattern to A (i.e. same indices and indptr)\n",
      "    \n",
      "    Returns\n",
      "    out_data, though note that this is the same parameter passed in and overwitten.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if A.dtype == np.float64:\n",
      "        sparseScalarProductOfSafeLnDot_f8(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    elif A.dtype == np.float32:\n",
      "        sparseScalarProductOfSafeLnDot_f4(A.data, A.indices, A.indptr, B, C, out.data)\n",
      "    else:\n",
      "        _sparseScalarProductOfSafeLnDot_py(A,B,C, out)\n",
      "    return out\n",
      "\n",
      "\n",
      "def _sparseScalarProductOfSafeLnDot_py(A,B,C, out=None):\n",
      "    '''\n",
      "    Calculates A * B.dot(C) where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based multiplication.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    out.data[:] = A.data\n",
      "    \n",
      "    rhs = B.dot(C)\n",
      "    rhs[rhs < sys.float_info.min] = sys.float_info.min\n",
      "    out.data *= np.log(rhs)[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def sparseScalarProductOf(A,B, out=None):\n",
      "    '''\n",
      "    Calculates A * B where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based multiplication.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if not out is A:\n",
      "        out.data[:] = A.data\n",
      "    out.data *= B[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def _sparseScalarQuotientOfDot_py(A,B,C, out=None):\n",
      "    '''\n",
      "    Calculates A / B.dot(C) where A is a sparse matrix\n",
      "    \n",
      "    Retains sparsity in the result, unlike the built-in operator\n",
      "    \n",
      "    Note the type of the return-value is the same as the type of\n",
      "    the sparse matrix A. If this has an integral type, this will\n",
      "    only provide integer-based division.\n",
      "    '''\n",
      "    if out is None:\n",
      "        out = A.copy()\n",
      "    if not out is A:\n",
      "        out.data[:] = A.data\n",
      "    \n",
      "    out.data /= B.dot(C)[csr_indices(out.indptr, out.indices)]\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "@autojit\n",
      "def csr_indices(ptr, ind):\n",
      "    '''\n",
      "    Returns the indices of a CSR matrix, given its indptr and indices arrays.\n",
      "    '''\n",
      "    rowCount = len(ptr) - 1 \n",
      "    \n",
      "    rows = [0] * len(ind)\n",
      "    totalElemCount = 0\n",
      "\n",
      "    for r in range(rowCount):\n",
      "        elemCount = ptr[r+1] - ptr[r]\n",
      "        if elemCount > 0:\n",
      "            rows[totalElemCount : totalElemCount + elemCount] = [r] * elemCount\n",
      "        totalElemCount += elemCount\n",
      "\n",
      "    return [rows, ind.tolist()]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##\u00a0Sparse Implementations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = vocab.astype(np.float64)\n",
      "\n",
      "gT = np.exp(M)     # D x T\n",
      "gU = gT.dot(vocab) # D x V\n",
      "gR = sparseScalarQuotientOfDot(Ws, gT, vocab)  # D x V   [W / U]\n",
      "gV = gT * (gR.dot(vocab.T)) # D x K\n",
      "\n",
      "vocabLen = vocab.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The stanard scalar equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = wordCounts\n",
      "\n",
      "sca1, sca2, sca3, sca4 = 0, 0, 0, 0\n",
      "\n",
      "# Line 1 from Guillaume's note\n",
      "sca1 -= sum((M[d,k]**2 + V[d,k]**2) * n[d] * lxi[d,k] \\\n",
      "           for k in range(K) for d in range(D))\n",
      "sca1 += sum(M[d,k] * (2 * n[d] * s[d] * lxi[d,k] - 0.5*n[d] \\\n",
      "            + sum(W[d,v] * z[d,v,k] for v in range(T))) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 2 from Guillaume's note\n",
      "sca2 += sum(safe_log(vocab[k,v]) * (sum (W[d,v] * z[d,v,k] for d in range(D))) for k in range(K) for v in range(T))\n",
      "sca2 -= sum(W[d,v] * sum(z[d,v,k] * safe_log(z[d,v,k]) for k in range(K)) \\\n",
      "           for v in range(T) for d in range(D))\n",
      "\n",
      "# Line 3 from Guillaume's note\n",
      "sca3 -= sum(n[d] * (\\\n",
      "            lxi[d,k]*(s[d]**2 - xi[d,k]**2) \\\n",
      "            - 0.5*(s[d] + xi[d,k]) \\\n",
      "            + log(1 + exp(xi[d,k])) \\\n",
      "            ) \\\n",
      "            for k in range(K) for d in range(D))\n",
      "\n",
      "# Line 4 omitted from Guillaume's note\n",
      "# In my derivation it's -sum_d sum_n sum_k z_dnk s_d = -\\sum_d n_d s_d\n",
      "sca4 -= sum(n[d] * s[d] for d in range(D))\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (sca1, sca2, sca3, sca4, sca1+sca2+sca3+sca4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-988.431067 + -627.348414 + -5560.977364 + -1097.675658 = -8274.432504\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now the sparse-optimized vectorized equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab64 = vocab.astype(np.float64)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec1, vec2, vec3, vec4 = 0, 0, 0, 0\n",
      "\n",
      "vec1 -= np.sum((M*M + V*V) * n[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * 2 * n[:,np.newaxis] * s[:,np.newaxis] * lxi)\n",
      "vec1 += np.sum(M * -0.5 * n[:,np.newaxis])\n",
      "vec1 += np.sum(M * gV)\n",
      "\n",
      "vec2 -= np.sum(M * gV) # Note the cancelation above\n",
      "vec2 -= -np.sum(sparseScalarProductOfSafeLnDot(Ws, gT, vocab64).data)\n",
      "\n",
      "vec3 -= np.sum(n[:,np.newaxis] * lxi * ((s*s)[:,np.newaxis] - (xi * xi)))\n",
      "vec3 += np.sum(0.5 * n[:,np.newaxis] * (s[:,np.newaxis] + xi))\n",
      "vec3 -= np.sum(n[:,np.newaxis] * np_safe_log(1 + np.exp(xi)))\n",
      "\n",
      "vec4 -= np.dot(s, n)\n",
      "\n",
      "print (\"%f + %f + %f + %f = %f\\n\" % (vec1, vec2, vec3, vec4, vec1+vec2+vec3+vec4))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-988.431067 + -627.348414 + -5560.977364 + -1097.675658 = -8274.432504\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Determinant Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = np.array( [[0.4, 0, 0, 0], [0, 13.43, 0, 0], [0, 0, 0.12, 0], [0, 0, 0, 3.1459]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "array([[  0.4   ,   0.    ,   0.    ,   0.    ],\n",
        "       [  0.    ,  13.43  ,   0.    ,   0.    ],\n",
        "       [  0.    ,   0.    ,   0.12  ,   0.    ],\n",
        "       [  0.    ,   0.    ,   0.    ,   3.1459]])"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.det(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "2.027972976"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.product(np.diag(test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 74,
       "text": [
        "2.027972976"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.log(la.det(test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "0.7070367601963653"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(np.log(np.diag(test)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "0.70703676019636541"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Update for the Topic Means"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "isigT = la.inv(sigT)\n",
      "\n",
      "d = 3\n",
      "v_d = 2 * s[d] * lxi[d,:] - 0.5 + 1./n[d] * gV[d,:]\n",
      "D_d = 2 * np.diag(lxi[d])\n",
      "lhs = la.inv(n[d] * D_d + isigT)\n",
      "rhs = n[d] * v_d + isigT.dot(mu)\n",
      "\n",
      "lhs.dot(rhs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vMat   = (2  * s[:,np.newaxis] * lxi - 0.5) * n[:,np.newaxis] + gV\n",
      "rhsMat = vMat + isigT.dot(mu)\n",
      "nD = n[d,np.newaxis] * 2 * lxi\n",
      "\n",
      "isigT\n",
      "means_d = la.inv(ssp.diags(nD[d,:], 0) + isigT).dot(rhsMat[d,:])\n",
      "means_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vMat   = (2  * s[:,np.newaxis] * lxi - 0.5) * n[:,np.newaxis] + gV\n",
      "rhsMat = vMat + isigT.dot(mu)\n",
      "\n",
      "isigT.flat[::K+1] += n[d] * 2 * lxi[d,:]\n",
      "means_d = la.inv(isigT).dot(rhsMat[d,:])\n",
      "isigT.flat[::K+1] -= n[d] * 2 * lxi[d,:]\n",
      "means_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rhsMat[d,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "array([-4.32225947, -3.80605519, -3.61484059, -3.76974105, -4.24516418,\n",
        "       -3.98314704])"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The complication is the left-hand side. It's a $K \\times K$ matrix for every document, and moreover, it's a matrix that needs to be _inverted_ for every document. Clearly, we really could do without this.\n",
      "\n",
      "What's truly awful though is that this requires that we use a `for` loop in Python over documents which is going to be dog slow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vMat   = (2  * s[:,np.newaxis] * lxi - 0.5) * n[:,np.newaxis] + gV\n",
      "rhsMat = vMat + isigT.dot(mu)\n",
      "nD = n[d,np.newaxis] * 2 * lxi\n",
      "\n",
      "isigT\n",
      "means_d = la.inv(ssp.diags(nD[d,:], 0) + isigT).dot(rhsMat[d,:])\n",
      "means_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lhs.dot(rhs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D_d = n[:,np.newaxis] * 2 * lxi[d,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.inv(ssp.diags(D_d[d,:], 0) + isigT).dot(rhs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 84,
       "text": [
        "array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812])"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rhs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "array([-4.32225947, -3.80605519, -3.61484059, -3.76974105, -4.24516418,\n",
        "       -3.98314704])"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rhsMat[d,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 86,
       "text": [
        "array([-4.32225947, -3.80605519, -3.61484059, -3.76974105, -4.24516418,\n",
        "       -3.98314704])"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "isigT.flat[::K+1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "array([ 1.16033412,  1.64416707,  0.27369607,  1.78989091,  1.39072421,\n",
        "        1.46148196])"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testMat = np.arange(25).reshape((5,5))\n",
      "testMat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "array([[ 0,  1,  2,  3,  4],\n",
        "       [ 5,  6,  7,  8,  9],\n",
        "       [10, 11, 12, 13, 14],\n",
        "       [15, 16, 17, 18, 19],\n",
        "       [20, 21, 22, 23, 24]])"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testMat.flat[::6] -= 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testMat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "array([[-2,  1,  2,  3,  4],\n",
        "       [ 5,  4,  7,  8,  9],\n",
        "       [10, 11, 10, 13, 14],\n",
        "       [15, 16, 17, 16, 19],\n",
        "       [20, 21, 22, 23, 22]])"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Double Checking Guillaume's Optimisation\n",
      "\n",
      "The derivation is $(2 n_d \\text{diag}(\\Lambda(\\xi_d) + \\Sigma^{-1})^{-1}(n_d V_d + \\Sigma^{-1}\\mu)$. I've reproduced this with the conditional model.\n",
      "\n",
      "The derivation presented in algorithm 2 is $(2 n_d \\text{diag}(\\Lambda(\\xi_d))\\Sigma + I_K)^{-1}(n_d \\Sigma V_d + \\mu)$\n",
      "\n",
      "Note that algorithm 2 doesn't require us to invert $\\Sigma$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "isigT = la.inv(sigT)\n",
      "\n",
      "d = 3\n",
      "v_d = 2 * s[d] * lxi[d,:] - 0.5 + 1./n[d] * gV[d,:]\n",
      "D_d = 2 * np.diag(lxi[d])\n",
      "lhs = la.inv(n[d] * D_d + isigT)\n",
      "rhs = n[d] * v_d + isigT.dot(mu)\n",
      "\n",
      "original = lhs.dot(rhs)\n",
      "\n",
      "lhs = la.inv(n[d] * D_d * sigT + ssp.eye(K))\n",
      "rhs = n[d] * sigT.dot(v_d) + mu\n",
      "later = lhs.dot(rhs)\n",
      "\n",
      "[original, later]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 91,
       "text": [
        "[array([-1.18086956, -0.94140159, -1.00665745, -1.18189246, -1.16945654,\n",
        "       -1.04361812]),\n",
        " array([-5.41234938, -5.61626808, -4.01549496, -4.96089168, -5.84306868,\n",
        "       -5.47439266])]"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So in summary, the alternate model, which was evidentally trying to avoid inverting $\\Sigma$, just doesn't work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Double Checking the Vocabulary Update"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The scalar version"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = vocab.copy()\n",
      "\n",
      "for k in range(K):\n",
      "    denom = 0\n",
      "    for v in range(T):\n",
      "        for d in range(D):\n",
      "            denom += z[d,v,k] * W[d,v]\n",
      "    for v in range(T):\n",
      "        tot = 0\n",
      "        for d in range(D):\n",
      "            tot += z[d,v,k] * W[d,v]\n",
      "        sca[k,v] = tot / denom\n",
      "\n",
      "(sca * 100).astype(np.int32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "array([[34, 33, 32,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0, 33, 34, 32,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0, 33, 33, 32],\n",
        "       [34,  0,  0, 32,  0,  0, 33,  0,  0],\n",
        "       [ 0, 32,  0,  0, 34,  0,  0, 33,  0],\n",
        "       [ 0,  0, 33,  0,  0, 33,  0,  0, 33]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "vec = vocab.copy()\n",
      "\n",
      "vec *= (gR.T.dot(gT)).T # Awkward order to maintain sparsity (R is sparse, expMeans is dense)\n",
      "vec = normalize_rows_ip(vocab)\n",
      "vec += 1E-300\n",
      "\n",
      "(vec * 100).astype(np.int32)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "array([[33, 33, 33,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0, 33, 33, 33,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0, 33, 33, 33],\n",
        "       [33,  0,  0, 33,  0,  0, 33,  0,  0],\n",
        "       [ 0, 33,  0,  0, 33,  0,  0, 33,  0],\n",
        "       [ 0,  0, 33,  0,  0, 33,  0,  0, 33]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Test the update for the variances\n",
      "\n",
      "First we start with the scalar approach"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = np.ndarray(shape=(D,K))\n",
      "\n",
      "for d in range(D):\n",
      "    for k in range(K):\n",
      "        sca[d,k] = 1./ (2 * n[d] * lxi[d,k] + isigT[k,k])\n",
      "\n",
      "sca[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "array([[ 0.1648914 ,  0.15284357,  0.19154453,  0.14665464,  0.16226906,\n",
        "         0.15908661],\n",
        "       [ 0.19644031,  0.17801046,  0.23521601,  0.17744418,  0.19496482,\n",
        "         0.18927098],\n",
        "       [ 0.13644378,  0.13224514,  0.15860147,  0.12760714,  0.131087  ,\n",
        "         0.13230723],\n",
        "       [ 0.20915554,  0.18623569,  0.2539185 ,  0.1894067 ,  0.20116159,\n",
        "         0.20247671],\n",
        "       [ 0.20374469,  0.17930654,  0.23962508,  0.17385605,  0.18743828,\n",
        "         0.18633071],\n",
        "       [ 0.25624333,  0.23118107,  0.33138701,  0.2203449 ,  0.24647999,\n",
        "         0.2378013 ],\n",
        "       [ 0.169534  ,  0.15347017,  0.18961699,  0.15297388,  0.16499059,\n",
        "         0.15456316],\n",
        "       [ 0.20624939,  0.19399776,  0.25588357,  0.18291808,  0.19500744,\n",
        "         0.19291437],\n",
        "       [ 0.16853152,  0.15010197,  0.1962202 ,  0.14560341,  0.15605344,\n",
        "         0.15273506],\n",
        "       [ 0.1266008 ,  0.12034027,  0.15161288,  0.12008405,  0.12940583,\n",
        "         0.12345111]])"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = 1./(2 * n[:,np.newaxis] * lxi + isigT.flat[::K+1])\n",
      "\n",
      "vec[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "array([[ 0.1648914 ,  0.15284357,  0.19154453,  0.14665464,  0.16226906,\n",
        "         0.15908661],\n",
        "       [ 0.19644031,  0.17801046,  0.23521601,  0.17744418,  0.19496482,\n",
        "         0.18927098],\n",
        "       [ 0.13644378,  0.13224514,  0.15860147,  0.12760714,  0.131087  ,\n",
        "         0.13230723],\n",
        "       [ 0.20915554,  0.18623569,  0.2539185 ,  0.1894067 ,  0.20116159,\n",
        "         0.20247671],\n",
        "       [ 0.20374469,  0.17930654,  0.23962508,  0.17385605,  0.18743828,\n",
        "         0.18633071],\n",
        "       [ 0.25624333,  0.23118107,  0.33138701,  0.2203449 ,  0.24647999,\n",
        "         0.2378013 ],\n",
        "       [ 0.169534  ,  0.15347017,  0.18961699,  0.15297388,  0.16499059,\n",
        "         0.15456316],\n",
        "       [ 0.20624939,  0.19399776,  0.25588357,  0.18291808,  0.19500744,\n",
        "         0.19291437],\n",
        "       [ 0.16853152,  0.15010197,  0.1962202 ,  0.14560341,  0.15605344,\n",
        "         0.15273506],\n",
        "       [ 0.1266008 ,  0.12034027,  0.15161288,  0.12008405,  0.12940583,\n",
        "         0.12345111]])"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 96,
       "text": [
        "array([[1, 2, 3],\n",
        "       [4, 5, 6],\n",
        "       [7, 8, 9]])"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A.flat[::4] += 4\n",
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 97,
       "text": [
        "array([[ 5,  2,  3],\n",
        "       [ 4,  9,  6],\n",
        "       [ 7,  8, 13]])"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Verifying the update for the topic covariance matrix\n",
      "\n",
      "The derivation of this is pretty straightforward: it's the sample covariance of the means, plus the sum of their individual, diagonal variances $V_d$. So\n",
      "\n",
      "$\\Sigma = \\frac{1}{D}\\sum_d \\left[ (m_d - \\mu)(m_d - \\mu)^{\\top} + V_d \\right]$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca = np.ndarray(shape=(K,K))\n",
      "sca.fill(0)\n",
      "\n",
      "for d in range(D):\n",
      "    dif = M[d,K] - mu\n",
      "    sca += dif.dot(dif.T)\n",
      "    sca += gV[d,:]\n",
      "\n",
      "sca"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "index 6 is out of bounds for axis 1 with size 6",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-99-54ebf243855b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msca\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msca\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bohning Bound Tests"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.eye(K) - 1./K"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(M[d,:].T.dot(A).dot(M[d,:]) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 105,
       "text": [
        "5.958715713878167"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.trace(M.dot((M.dot(A)).T))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 108,
       "text": [
        "5.9587157138781661"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(M * M.dot(A))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 109,
       "text": [
        "5.9587157138781688"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Dot Product of Vectors and their Softmax"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pure Python\n",
      "\n",
      "def normalizerows_ip (matrix):\n",
      "    '''\n",
      "    Normalizes a matrix IN-PLACE.\n",
      "    '''\n",
      "    row_sums = matrix.sum(axis=1)\n",
      "    matrix   /= row_sums[:, np.newaxis]\n",
      "    return matrix\n",
      "\n",
      "def rowwise_softmax (matrix):\n",
      "    '''\n",
      "    Assumes each row of the given matrix is an unnormalized distribution and\n",
      "    uses the softmax metric to normalize it. This additionally uses some\n",
      "    scaling to ensure that we never overflow.\n",
      "    '''\n",
      "    # TODO Just how compute intense is this method call?\n",
      "    \n",
      "    row_maxes = matrix.max(axis=1) # Underflow makes sense i.e. Pr(K=k) = 0. Overflow doesn't, i.e Pr(K=k) = \\infty\n",
      "    result    = np.exp(matrix - row_maxes[:, np.newaxis])\n",
      "    result   /= result.sum(axis=1)[:,np.newaxis]\n",
      "    return result\n",
      "\n",
      "def selfSoftDot_vec(x):\n",
      "    lse = np.sum(np.exp(x))\n",
      "    sx  = np.exp(x) / lse\n",
      "    return x.dot(sx)\n",
      "\n",
      "def selfSoftDot_long(X):\n",
      "    return sum(selfSoftDot_vec(X[d,:]) for d in range(X.shape[0]))\n",
      "\n",
      "# This is likely very fast - the primary cost is memory\n",
      "def selfSoftDot(X):\n",
      "    sigX = rowwise_softmax(X)\n",
      "    return np.sum(sigX * X)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython  -c=-msse -c=-msse2 -c=-msse3 -c=-march=native -c=-mfpmath=sse\n",
      "\n",
      "cimport cython\n",
      "import numpy as np\n",
      "cimport numpy as np\n",
      "from libc.math cimport exp, log\n",
      "from libc.float cimport FLT_MIN, DBL_MIN\n",
      "\n",
      "# This may be slower than the numpy/Python version,\n",
      "# but is constant in memory use\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def scaledSelfSoftDot_cy(double[:,:] mat, double[:] scale):\n",
      "    cdef:\n",
      "        double total = 0.0\n",
      "        double lse = 0.0\n",
      "        int rows = mat.shape[0]\n",
      "        int cols = mat.shape[1]\n",
      "        int row = 0\n",
      "        int col = 0\n",
      "    \n",
      "    with nogil:\n",
      "        for row in range(rows):\n",
      "            expSum = 0.0\n",
      "            for col in range(cols):\n",
      "                expSum += exp(mat[row,col])\n",
      "        \n",
      "            for col in range(cols):\n",
      "                total += scale[row] * exp(mat[row,col]) * mat[row,col] / expSum\n",
      "    \n",
      "    return total\n",
      "\n",
      "# This may be slower than the numpy/Python version,\n",
      "# but is constant in memory use\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.cdivision(True)\n",
      "def selfSoftDot_cy(double[:,:] mat):\n",
      "    cdef:\n",
      "        double total = 0.0\n",
      "        double lse = 0.0\n",
      "        int rows = mat.shape[0]\n",
      "        int cols = mat.shape[1]\n",
      "        int row = 0\n",
      "        int col = 0\n",
      "    \n",
      "    with nogil:\n",
      "        for row in range(rows):\n",
      "            expSum = 0.0\n",
      "            for col in range(cols):\n",
      "                expSum += exp(mat[row,col])\n",
      "        \n",
      "            for col in range(cols):\n",
      "                total += exp(mat[row,col]) * mat[row,col] / expSum\n",
      "    \n",
      "    return total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "selfSoftDot_cy(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 143,
       "text": [
        "17.649568285742664"
       ]
      }
     ],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "selfSoftDot_long(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 144,
       "text": [
        "17.649568285742653"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "selfSoftDot(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 145,
       "text": [
        "17.649568285742664"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n 500 selfSoftDot_long(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "500 loops, best of 3: 3.04 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n 500 selfSoftDot(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "500 loops, best of 3: 102 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n 10000 selfSoftDot_cy(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 14.2 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bound on Z_dvk\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.eye(K) - 1./K\n",
      "(A * 100).astype(np.int32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 155,
       "text": [
        "array([[ 83, -16, -16, -16, -16, -16],\n",
        "       [-16,  83, -16, -16, -16, -16],\n",
        "       [-16, -16,  83, -16, -16, -16],\n",
        "       [-16, -16, -16,  83, -16, -16],\n",
        "       [-16, -16, -16, -16,  83, -16],\n",
        "       [-16, -16, -16, -16, -16,  83]], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The simple first term. Note that the sum over v of $Z_{dvk}$ and $w_{dv}$ is just the count of words in document $d$ assigned to topic $k$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca1 = sum(sum(sum(M[d,k]*z[d,v,k]*W[d,v] for k in range(K)) \\\n",
      "               for v in range(T))  for d in range(D))\n",
      "sca1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 196,
       "text": [
        "265.75530518735985"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec1 = np.sum(M * gV)\n",
      "vec1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "265.75530518735991"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The bound on the LSE, first term"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sca2 = sum(2 * n[d] * M[d,:].T.dot(A).dot(M[d,:]) for d in range(D))\n",
      "sca2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 180,
       "text": [
        "225.88307699718024"
       ]
      }
     ],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec2 = np.sum(2 * ssp.diags(n,0) * M.dot(A) * M)\n",
      "vec2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 184,
       "text": [
        "225.88307699718013"
       ]
      }
     ],
     "prompt_number": 184
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The bound on the LSE, second term"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(vec):\n",
      "    denom = np.sum(np.exp(vec))\n",
      "    return np.exp(vec) / denom\n",
      "\n",
      "sca3 = sum(- n[d] * 2 * softmax(M[d,:]).T.dot(M[d,:]) for d in range(D))\n",
      "sca3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 185,
       "text": [
        "-702.67079215605338"
       ]
      }
     ],
     "prompt_number": 185
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec3 = -2 * scaledSelfSoftDot_cy(M, n.astype(M.dtype))\n",
      "vec3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 191,
       "text": [
        "-702.6707921560535"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The bound on the LSE, third term"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(-0.5 * n[d] * np.trace(np.diag(V[d,:]).dot(A)) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 212,
       "text": [
        "-2473.5590189252189"
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-0.5 * np.sum(n[:,np.newaxis] * V * (np.diag(A))[np.newaxis,:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 213,
       "text": [
        "-2473.5590189252189"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The bound on the LSE, final term"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lse(vec):\n",
      "    return np.log(np.sum(np.exp(vec)))\n",
      "\n",
      "sum(n[d] * lse(M[d,:]) for d in range(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 217,
       "text": [
        "3912.6748389146869"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(n * np.log(np.sum(np.exp(M), axis=1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 219,
       "text": [
        "3912.6748389146869"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}