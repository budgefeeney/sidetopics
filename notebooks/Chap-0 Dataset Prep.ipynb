{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "\n",
    "In this chapter we investigate a number of basic questions\n",
    "\n",
    "1. The ability of LDA to generalise to increasinly large numbers of topics, versus a mixture of multinomials, by comparing MoM/VB and MoM/Gibbs with LDA/VB and LDA/Gibbs\n",
    "2. The ability of the CVB and CVB0 algorithm to outperform the LDA/VB and be competitive with LDA/Gibbs algorithm on a the same three datasets as (1)\n",
    "3. The comparison of three concentration parameters with HDP on one dataset, verse the LDA/VB spread on all\n",
    "4. The effectiveness of the different evaluation techniques on 4-news using LDA/VB and MoM/VB\n",
    "5. Perhaps a comparison of the impact of different ways of estimating perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outstanding issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [x] Locate files\n",
    "1. [x] Find dictionary for NIPS\n",
    "1. [x] Does every NIPS feature matrix have an intercept. If so, where is it?\n",
    "1. [ ] What is the \"4\" variant of NIPS\n",
    "1. [x] Regen Reuters corpora and verify (with tags and doc-ids)\n",
    "1. [x] Regen 20-news corpus and verify\n",
    "1. [x] Regen NIPS corpus excluding references\n",
    "1. [ ] Regen ACL\n",
    "1. [ ] Find dictionary for Arxiv (or regen it)\n",
    "1. [x] Find right version of 20News \n",
    "1. [x] See if the IDs are really necessary to align the various matrices\n",
    "1. [x] Figure out ACL vs Arxiv\n",
    " * The trick is using symbolic links to make ACL corpus pipeline run on the extracted Arxiv data. It's quite rough.\n",
    " * No word on NIPs however\n",
    " * Also ACL feature extraction was terrible\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "_ = logging.getLogger() # In Jupyter you need at least one throwaway instance\n",
    "logging.basicConfig(    # for basicConfig changes to stick, not sure about cmdline\n",
    "    format='%(asctime)s  %(levelname)-7s %(module)s::%(funcName)s() - %(message)s',\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(pathlib.Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as ssp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import NamedTuple, Any, List, Dict, Set, Union\n",
    "import pickle as pkl\n",
    "# import _pickle as cpkl\n",
    "import six; \n",
    "from six.moves import cPickle as cpkl\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import logging\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = pathlib.Path('/') / 'Volumes' / 'DatasetSSD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelledMatrix:\n",
    "    values: Union[ssp.csr_matrix, np.ndarray]\n",
    "    labels: Any = None\n",
    "        \n",
    "    def __init__(self, values, labels=None):\n",
    "        self.values = values\n",
    "        self.labels = labels\n",
    "        \n",
    "        if self.values is not None and self.labels is not None:\n",
    "            assert(self.values.shape[1] == len(self.labels)), \\\n",
    "                f\"Matrix has shape {self.values.shape} but labels has length {len(self.labels)}\"\n",
    "            \n",
    "    def __str__(self):\n",
    "        l = \"\" if self.labels is None else \"(labelled)\"\n",
    "        return f\"[{self.values.shape}]{l}\"\n",
    "    \n",
    "class RawData(NamedTuple):\n",
    "    words: LabelledMatrix\n",
    "    feats: LabelledMatrix = None\n",
    "    cites: LabelledMatrix = None\n",
    "    authors: LabelledMatrix = None\n",
    "    categories: LabelledMatrix = None\n",
    "    row_labels: List[str] = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"RawData(words{self.words}, feats{self.feats}, cites{self.cites}, authors{self.authors}, \" \\\n",
    "               f\"categories{self.categories}, row_labels[{'' if self.row_labels is None else len(self.row_labels)}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clearning\n",
    "\n",
    "## Ensure we have Category Information for all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l: List[List[Any]]) -> List[Any]:\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(nltk_metadata_file: pathlib.Path, fileIds: List[str]) -> List[List[str]]:\n",
    "    if nltk_metadata_file.exists():\n",
    "        logging.info(f\"Reading metadata from {nltk_metadata_file}\")\n",
    "        row_cats = {}\n",
    "        with open(nltk_metadata_file, 'r') as f:\n",
    "            for line in f:\n",
    "                vals = line.split(' ')\n",
    "                row_cats[vals[0]] = vals[1:]\n",
    "            return flatten(row_cats[word_row_id] for word_row_id in fileIds)\n",
    "    else:\n",
    "        logging.info(f\"No such metadata file {nltk_metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickled_id_cat_word_metadata(\n",
    "    nltk_input_dir: pathlib.Path,\n",
    "    processed_input_dir: pathlib.Path,\n",
    "    output_input_dir: pathlib.Path = None) -> None:\n",
    "    if output_input_dir is None:\n",
    "        output_input_dir = processed_input_dir\n",
    "    \n",
    "    logging.info(\"Reading in IDs and word dictionary\")\n",
    "    exec(open(processed_input_dir / \"ids-words.py\").read())\n",
    "    exec(open(processed_input_dir / \"words.py\").read())\n",
    "\n",
    "    logging.info(\"Writing out row IDs and dictionary entries\")\n",
    "    with open(processed_input_dir / 'fileIds.pkl', 'wb') as f:\n",
    "        pkl.dump(locals().get('fileIds'), f)\n",
    "    with open(processed_input_dir / 'words_dict.pkl', 'wb') as f:\n",
    "        pkl.dump(locals().get('words_dict'), f)\n",
    "        \n",
    "    row_cats = read_metadata(nltk_input_dir / 'cats.txt', locals().get('fileIds'))\n",
    "    if row_cats:\n",
    "        with open(processed_input_dir / 'cats.pkl', 'wb') as f:\n",
    "            pkl.dump(row_cats, f)\n",
    "    \n",
    "    row_authors = read_metadata(nltk_input_dir / 'authors.txt', locals().get('fileIds'))\n",
    "    if row_authors:\n",
    "        with open(processed_input_dir / 'authors.pkl', 'wb') as f:\n",
    "            pkl.dump(row_authors, f)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twenty News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickled_id_cat_word_metadata(\n",
    "    nltk_input_dir=pathlib.Path.home() / 'Downloads' / 'words-only' / 'Raw Data' / 'TwentyNewsClean',\n",
    "    processed_input_dir=DATASET_DIR / 'words-only' / '20news4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickled_id_cat_word_metadata(\n",
    "    nltk_input_dir=pathlib.Path.home() / 'Downloads' / 'words-only' / 'Raw Data' / 'reuters',\n",
    "    processed_input_dir=DATASET_DIR / 'words-only' / 'reuters'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickled_id_cat_word_metadata(\n",
    "    nltk_input_dir=pathlib.Path.home() / 'Downloads' / 'words-only' / 'Raw Data' / 'nips',\n",
    "    processed_input_dir=DATASET_DIR / 'words-only' / 'nips'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWENTY_NEWS_DIR = DATASET_DIR / '20news4'\n",
    "TWENTY_NEWS_WORDS_FILE = TWENTY_NEWS_DIR / 'words.pkl'\n",
    "TWENTY_NEWS_DICT_FILE = TWENTY_NEWS_DIR / 'dict.pkl'\n",
    "\n",
    "# Create categories.pkl if it doesn't exist\n",
    "\n",
    "for p in [TWENTY_NEWS_WORDS_FILE, TWENTY_NEWS_DICT_FILE]:\n",
    "    assert p.exists(), p\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twenty_news() -> RawData:\n",
    "    words = cpkl.load(gzip.GzipFile(TWENTY_NEWS_WORDS_FILE, 'rb'))\n",
    "    with open(TWENTY_NEWS_DICT_FILE, 'rb') as f:\n",
    "        dic = pkl.load(f)\n",
    "    return RawData(words=LabelledMatrix(values=words, labels=dic))\n",
    "\n",
    "_t = twenty_news()\n",
    "str(_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACL_DIR = DATASET_DIR / 'ACL' / 'ACL.100.clean'\n",
    "ACL_WORDS_FILE = ACL_DIR / 'words-freq.pkl'\n",
    "ACL_DICT_FILE = ACL_DIR / 'words-freq-dict.pkl'\n",
    "ACL_REF_FILE = ACL_DIR / 'ref.pkl'\n",
    "ACL_FEATS_FILE = ACL_DIR / 'feats.pkl'\n",
    "ACL_FEATS_DICT_FILE = ACL_DIR / 'feats_dict.pkl'\n",
    "ACL_DOC_IDS_FILE = ACL_DIR / 'doc_ids.pkl'\n",
    "\n",
    "for p in [ACL_WORDS_FILE, ACL_DICT_FILE, ACL_REF_FILE, ACL_FEATS_FILE, ACL_FEATS_DICT_FILE]:\n",
    "    assert p.exists(), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acl() -> RawData:\n",
    "    with open(ACL_WORDS_FILE, 'rb') as f:\n",
    "        words = pkl.load(f)\n",
    "    with open(ACL_DICT_FILE, 'rb') as f:\n",
    "        words_dict = pkl.load(f)\n",
    "        \n",
    "    with open(ACL_REF_FILE, 'rb') as f:\n",
    "        refs = pkl.load(f)\n",
    "        \n",
    "    with open(ACL_FEATS_FILE, 'rb') as f:\n",
    "        feats = pkl.load(f)\n",
    "    with open(ACL_FEATS_DICT_FILE, 'rb') as f:\n",
    "        feats_dict = pkl.load(f)\n",
    "        \n",
    "    with open (ACL_DOC_IDS_FILE, 'rb') as f:\n",
    "        doc_ids = pkl.load(f)\n",
    "        \n",
    "    assert words.shape[0] == refs.shape[0] == feats.shape[0] == len(doc_ids)\n",
    "    \n",
    "    return RawData(\n",
    "        words=LabelledMatrix(values=words, labels=words_dict),\n",
    "        feats=LabelledMatrix(values=feats, labels=feats_dict),\n",
    "        cites=LabelledMatrix(values=refs, labels=doc_ids),\n",
    "        row_labels=doc_ids\n",
    "    )\n",
    "\n",
    "_a = acl()\n",
    "str(_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV_DIR = DATASET_DIR / 'Arxiv'\n",
    "ARXIV_CITES_FILE = ARXIV_DIR / 'cites.pkl'\n",
    "ARXIV_CITES_DICT_FILE = ARXIV_DIR / 'cites.py.pkl'\n",
    "ARXIV_WORDS_FILE = ARXIV_DIR / 'words.pkl'\n",
    "ARXIV_FEATS_FILE = ARXIV_DIR / 'feats.pkl'\n",
    "ARXIV_FEATS_DICT_FILE = ARXIV_DIR / 'feats.py.pkl'\n",
    "ARXIV_DOC_IDS_FILE = ARXIV_DIR / 'doc_ids.pkl'\n",
    "ARXIV_DICT_FILE = None\n",
    "\n",
    "for p in [ARXIV_CITES_FILE, ARXIV_WORDS_FILE, ARXIV_FEATS_FILE, ARXIV_DICT_FILE]:\n",
    "    assert p.exists(), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv() -> RawData:\n",
    "    with open(ARXIV_WORDS_FILE, 'rb') as f:\n",
    "        words = pkl.load(f)\n",
    "    words_dict = None\n",
    "        \n",
    "    with open(ARXIV_CITES_FILE, 'rb') as f:\n",
    "        refs = pkl.load(f)\n",
    "    with open(ARXIV_CITES_DICT_FILE, 'rb') as f:\n",
    "        refs_dict = pkl.load(f)\n",
    "        \n",
    "    with open(ARXIV_FEATS_FILE, 'rb') as f:\n",
    "        feats = pkl.load(f)\n",
    "    with open(ARXIV_FEATS_DICT_FILE, 'rb') as f:\n",
    "        feats_dict = pkl.load(f)\n",
    "        \n",
    "    with open(ARXIV_DOC_IDS_FILE, 'rb') as f:\n",
    "        doc_ids = pkl.load(f)\n",
    "        \n",
    "    assert words.shape[0] == refs.shape[0] == feats.shape[0] == len(doc_ids), \\\n",
    "        f\"words.shape = {words.shape}, refs.shape={refs.shape}, feats.shape={feats.shape}\" \\\n",
    "        f\"len(doc_ids) = {len(doc_ids)}\"\n",
    "    \n",
    "    return RawData(\n",
    "        words=LabelledMatrix(values=words, labels=words_dict),\n",
    "        feats=LabelledMatrix(values=feats, labels=feats_dict),\n",
    "        cites=LabelledMatrix(values=refs, labels=refs_dict),\n",
    "        row_labels=doc_ids\n",
    "    )\n",
    "\n",
    "_x = arxiv()\n",
    "str(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIPS_DIR = DATASET_DIR / 'NIPS'\n",
    "NIPS_WORDS_FILE = NIPS_DIR / 'words.pkl'\n",
    "NIPS_DICT_FILE = NIPS_DIR / 'dict.pkl'\n",
    "NIPS_AUTHORS_FILE = NIPS_DIR / 'authors.pkl'\n",
    "NIPS_AUTHORS_DICT_FILE = NIPS_DIR / 'authors.dict'\n",
    "NIPS_AUTHORS_FILE_4 = NIPS_DIR / 'authors4.pkl'\n",
    "NIPS_REFS_FILE = NIPS_DIR / 'refs.pkl'\n",
    "NIPS_REFS_DICT_FILE = NIPS_DIR / 'refs.dict'\n",
    "NIPS_REFS_FILE_4 = NIPS_DIR / 'refs4.pkl'\n",
    "NIPS_CATS_FILE = NIPS_DIR / 'cats.pkl'\n",
    "NIPS_CATS_DICT_FILE = NIPS_DIR / 'cats.dict'\n",
    "\n",
    "# From the \"dicts.py\" fle.\n",
    "NIPS_AUTHORS_DICT_FILE = NIPS_AUTHORS_DICT_FILE.parent / (NIPS_AUTHORS_DICT_FILE.name + \".patched\")\n",
    "NIPS_CATS_DICT_FILE = NIPS_CATS_DICT_FILE.parent / (NIPS_CATS_DICT_FILE.name + \".patched\")\n",
    "NIPS_REFS_DICT_FILE = NIPS_REFS_DICT_FILE.parent / (NIPS_REFS_DICT_FILE.name + \".patched\")\n",
    "\n",
    "for p in [NIPS_WORDS_FILE, NIPS_DICT_FILE,\n",
    "          NIPS_AUTHORS_FILE, NIPS_AUTHORS_DICT_FILE, NIPS_AUTHORS_FILE_4,\n",
    "          NIPS_REFS_FILE, NIPS_REFS_DICT_FILE, NIPS_REFS_FILE_4,\n",
    "          NIPS_CATS_FILE, NIPS_CATS_DICT_FILE]:\n",
    "    assert p.exists(), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nips() -> RawData:\n",
    "    with open(NIPS_WORDS_FILE, 'rb') as f:\n",
    "        words = pkl.load(f)\n",
    "    with open(NIPS_DICT_FILE, 'rb') as f:\n",
    "        words_dict = pkl.load(f)\n",
    "    logging.info(f\"words.shape = {words.shape}, len(words_dict) = {len(words_dict)})\")\n",
    "        \n",
    "    with open(NIPS_AUTHORS_FILE, 'rb') as f:\n",
    "        authors = pkl.load(f)\n",
    "    with open(NIPS_AUTHORS_DICT_FILE, 'rb') as f:\n",
    "        authors_dict = pkl.load(f)\n",
    "    authors_dict = [f'auth-{a}' for a in authors_dict]\n",
    "    logging.info(f\"authors.shape = {authors.shape}, len(authors_dict) = {len(authors_dict)})\")\n",
    "        \n",
    "    with open(NIPS_CATS_FILE, 'rb') as f:\n",
    "        cats = pkl.load(f)\n",
    "    with open(NIPS_CATS_DICT_FILE, 'rb') as f:\n",
    "        cats_dict = pkl.load(f)\n",
    "    cats_dict = [f'cat-{c}' for c in cats_dict]\n",
    "    logging.info(f\"cats.shape = {cats.shape}, len(cats_dict) = {len(cats_dict)})\")\n",
    "        \n",
    "    with open(NIPS_REFS_FILE_4, 'rb') as f:\n",
    "        refs = pkl.load(f)\n",
    "    with open(NIPS_REFS_DICT_FILE, 'rb') as f:\n",
    "        refs_dict = pkl.load(f)\n",
    "    refs_dict = [f'ref-{r}' for r in refs_dict]\n",
    "    logging.info(f\"refs.shape = {refs.shape}, len(refs_dict) = {len(refs_dict)})\")\n",
    "        \n",
    "    feats = ssp.hstack((authors, cats, refs))\n",
    "    feats_dict = authors_dict + cats_dict + refs_dict\n",
    "    \n",
    "    return RawData(\n",
    "        words=LabelledMatrix(values=words, labels=words_dict),\n",
    "        feats=LabelledMatrix(values=feats, labels=feats_dict)\n",
    "    )\n",
    "\n",
    "_n = nips()\n",
    "str(_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REUTERS_DIR = DATASET_DIR / 'reuters'\n",
    "REUTERS_WORDS_FILE = REUTERS_DIR / 'W.pkl'\n",
    "REUTERS_DICT_FILE = REUTERS_DIR / 'dict.pkl'\n",
    "\n",
    "for p in [REUTERS_DIR, REUTERS_WORDS_FILE, REUTERS_DICT_FILE]:\n",
    "    assert p.exists(), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuters() -> RawData:\n",
    "    with open(REUTERS_WORDS_FILE, 'rb') as f:\n",
    "        words = pkl.load(f)\n",
    "    with open(REUTERS_DICT_FILE, 'rb') as f:\n",
    "        words_dict = pkl.load(f)\n",
    "        \n",
    "    return RawData(words=LabelledMatrix(values=words, labels=words_dict))\n",
    "\n",
    "_r = reuters()\n",
    "str(_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_r.words.values[95,:].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(100):\n",
    "    print(f\"{r:03d}  {np.percentile(a=_r.words.values[r,:].data, q=[i/0.1 for i in range(11)]).astype(np.int32)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_r.words.values.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
