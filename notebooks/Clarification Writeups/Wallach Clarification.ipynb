{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wallach Clarification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did Hannah Wallach say?\n",
    "\n",
    " * Well she's thinking of T topics, and I guess ? words so here component distribution is  $\\Phi \\in \\mathbb{R}^{T \\times ?}$ with prior $\\text{Dir}(\\phi_t; \\beta \\boldsymbol{n})$\n",
    " * For each of the $D$ documents there's a topic distribution $\\theta_d$ with prior $\\text{Dir}(\\theta_d; \\alpha \\boldsymbol{m})$\n",
    "\n",
    "Finally, she notes the Polya identity, allowing the marginalisation of most parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She then moves out into how to evaluate the probability of some held out documents $W$ given some training documents $W'$ which is\n",
    "\n",
    "$$\n",
    "p(W | W') = \\int d\\Phi d\\alpha d\\boldsymbol{m}\n",
    "             \\text{ } p(W | \\Phi, \\alpha, \\boldsymbol{m}) \\text{ } p(\\Phi, \\alpha, \\boldsymbol{m}|W')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to note here is she has already margnalised out $\\Theta$ for the new documents. She assumes you learn the \"global\" parameters -- priors and component distribution -- and then fix these and use them to evaluate the new documents\n",
    "\n",
    "> So we have to think about what we're doing here. A mixture model is a good case. You can just directly evaluate the log likelihood $p(w|\\alpha, \\Phi) = \\sum_k p(w | \\phi_k)p(z=k|\\alpha)$. Or you can determine the posterior over clusters and use that to evaluate... except that it doesn't decompose $p(w|\\alpha, \\Phi) = \\sum_k p(w, z=k|\\alpha, \\Phi) = p(z=k|w, \\alpha, \\Phi)p(w|\\ldots)$. But it seems obvious to see how well you can \"explain\" documents: this is what doc-completion does. Hence it should be introduced in the clustering section. It's also a good metric to use if you want to consider the predictive ability to, e.g. predict hashtags.\n",
    "\n",
    "Now either way, you have to make a choice about your parameters. Are you using the _distribution_ over the parameters, or are you just taking a point estimate?\n",
    "\n",
    "1. Drawing samples from the parameter posterior and taking an average to evaluate the integral, i.e.  $\\mathbb{E}_{p(\\Phi, \\alpha, \\boldsymbol{m}|W')}\\left[ p(W | \\Phi, \\alpha, \\boldsymbol{m}) \\right]$. \n",
    "    * Stick a log in that expectation and you can start thinking about a variational approximation.\n",
    "2. Taking a point estimate of -- I guess $\\Phi, \\alpha, \\boldsymbol{m}$ -- and then use that to approximate\n",
    "\n",
    "The paper is concerned with point estimates. So where's the uncertainty.... Apparently its in $p(\\boldsymbol{w}_d | \\alpha \\boldsymbol{m}, \\Phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is that we've marginalised out $\\theta$ for each of the inference documents. We need this too. If you hold $\\Phi$ fixed (and so let it be found by any inference method), you can use Gibbs sampling to quickly get a distribution over $z$ and thereby, $\\theta$.\n",
    "\n",
    " * This is used by many methods she describes, being: FIXME\n",
    " * There are other methods that do not require this, being: FIXME\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $p(w|\\Phi, a \\boldsymbol{m})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Using Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hence there are two options:\n",
    "\n",
    "Directly sample $\\theta \\sim Dir(\\alpha \\boldsymbol{m})$ and average over all settings. But importance sampling doesn't work well in high-dimensions: it has high-variance, indeed, infinite variance with real-values high-dim values.\n",
    "\n",
    "The other is to choose a proposal distribution and weight such samples in the usual importance-sampling way. The proposal distribution is in fact a method for evaluating the posterior $p(z|w, \\alpha \\boldsymbol{m}, \\Phi)$\n",
    "\n",
    "$$\n",
    "\\theta^0 \\propto \\left(\\alpha \\boldsymbol{m}\\right) \\text{.* } \\Phi_{\\cdot, w_{n}} \n",
    "$$\n",
    "\n",
    "Which is just the prior over topics and the probability of words under each topic, i.e. $p(z = k| w, \\Phi, \\alpha \\boldsymbol{m}) \\propto p(w|Phi, z=k)p(z=k| \\alpha \\boldsymbol{m})$\n",
    "\n",
    "To draw samples, simply iterate\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{for }& s = 0 \\ldots S \\\\\n",
    " & z_n^{(s)} \\sim \\text{Mul}(\\theta^{(s)}, 1) \\\\\n",
    " & \\theta^{(s+1)} \\propto \\left(\\alpha \\boldsymbol{m} + \\sum_{n' \\neq n} \\theta^{(s)} \\text{.* } \\boldsymbol{\\bar{z}}_{n'}\\right) \\Phi_{\\cdot, w_{n}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(Recall that in more normal notation $\\alpha \\boldsymbol{m} = \\boldsymbol{\\alpha}$ and parameterises the prior. Also $z_n$ is the scalar and $\\bar{\\boldsymbol{z}}_n$ is the indicator vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Harmonic Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gibbs sampling to get a _posterior_ distribution over $z_n^s$.\n",
    "\n",
    "Then instead of using that to materlise an estimate of $\\theta$ (WHY), use it directly to figure out $p(w | \\alpha \\boldsymbol{m}, \\Phi)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
