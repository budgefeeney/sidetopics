{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Results\n",
    "\n",
    "**Objective:** Basic illustrative statistics to demonstrate the background.\n",
    "\n",
    "**Method**: Using LDA/Gibbs, LDA/VB, LDA/CVB MoM/Gibbs and MoM/VB demonstrate how well topic models work. Using LDA/VB and HDP demonstrate how well HDP finds ideal topic amounts. Using LDA/VB and online LDA (just use the version [packaged with sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) demonstrate how online learning helps expedite learning on fast datasets. Using LDA and MoM on Reuters and 20News, use labels to demonstrate other evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(pathlib.Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sidetopics.model.common import DataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = pathlib.Path('/') / 'Volumes' / 'DatasetSSD'\n",
    "CLEAN_DATASET_DIR = DATASET_DIR / 'words-only'\n",
    "\n",
    "T20_NEWS_DIR = CLEAN_DATASET_DIR / '20news4'\n",
    "NIPS_DIR = CLEAN_DATASET_DIR / 'nips'\n",
    "REUTERS_DIR = CLEAN_DATASET_DIR / 'reuters'\n",
    "\n",
    "TRUMP_WEEKS_DIR = DATASET_DIR / 'TrumpDb'\n",
    "NUS_WIDE_DIR = DATASET_DIR / 'NusWide'\n",
    "\n",
    "CITHEP_DATASET_DIR = DATASET_DIR / 'Arxiv'\n",
    "ACL_DATASET_DIR = DATASET_DIR / 'ACL' / 'ACL.100.clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t20_news = DataSet.from_files(words_file=T20_NEWS_DIR / 'words.pkl')\n",
    "reuters = DataSet.from_files(words_file=REUTERS_DIR / 'words.pkl')\n",
    "acl = DataSet.from_files(words_file=ACL_DATASET_DIR / 'words.pkl')\n",
    "arxiv = DataSet.from_files(words_file=CITHEP_DATASET_DIR / 'words.pkl')\n",
    "nips = DataSet.from_files(words_file=NIPS_DIR / 'words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t20_news.convert_to_dtype(DTYPE)\n",
    "reuters.convert_to_dtype(DTYPE)\n",
    "acl.convert_to_dtype(DTYPE)\n",
    "arxiv.convert_to_dtype(DTYPE)\n",
    "nips.convert_to_dtype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "| Dataset | Document Count | Total Words | Vocabulary Size | DocLen (Min) | DocLen (25) | DocLen (50) | DocLen (75) | DocLen (Max) |\n",
       "| ------- | -------------- | ----------- | --------------- | ------------ | ----------- | ----------- | ----------- | ------------ |\n",
       "| Reuters-21578 | 10,788 | 922,811 | 7729 | 4 | 29 | 56 | 105 | 999 |\n",
       "| 20-News | 18,821 | 3,029,297 | 20835 | 1 | 57 | 98 | 168 | 7,393 |\n",
       "| NIPS | 1,740 | 2,543,236 | 10422 | 19 | 1,272 | 1,495 | 1,724 | 4,773 |\n",
       "| ACL | 13,554 | 41,009,480 | 107954 | 100 | 2,080 | 3,022 | 3,689 | 19,238 |\n",
       "| Arxiv | 543 | 1,838,163 | 5563 | 12 | 1,614 | 2,504 | 3,875 | 43,606 |\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def corpus_stats(dataset: DataSet) -> str:\n",
    "    quarts = np.percentile(a=dataset.words.sum(axis=1), q=[0, 25, 50, 75, 100]).astype(np.int32)\n",
    "    quarts_str = ' | '.join(f'{q:,}' for q in quarts)\n",
    "    return f'{dataset.doc_count:,} | {int(dataset.word_count):,} | {dataset.words.shape[1]} | {quarts_str}'\n",
    "\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "\n",
    "| Dataset | Document Count | Total Words | Vocabulary Size | DocLen (Min) | DocLen (25) | DocLen (50) | DocLen (75) | DocLen (Max) |\n",
    "| ------- | -------------- | ----------- | --------------- | ------------ | ----------- | ----------- | ----------- | ------------ |\n",
    "| Reuters-21578 | {corpus_stats(reuters)} |\n",
    "| 20-News | {corpus_stats(t20_news)} |\n",
    "| NIPS | {corpus_stats(nips)} |\n",
    "| ACL | {corpus_stats(acl)} |\n",
    "| Arxiv | {corpus_stats(arxiv)} |\n",
    "\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to emphasise here is that we're deliberately chose a mix of datasets with small document lengths and large document lengths to look into overparameterisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "# rcv1 = fetch_rcv1()  a log TF-IDF rep, not much good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 1: MoM vs LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = reuters.cross_valid_split(test_fold_id=0, num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, valid = test.doc_completion_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import HdpTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Calculate approximate log-likelihood as score.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
       "    Document word matrix.\n",
       "\n",
       "y : Ignored\n",
       "\n",
       "Returns\n",
       "-------\n",
       "score : float\n",
       "    Use approximate bound as score.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Documents/GitHub/sidetopics/env/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LatentDirichletAllocation.score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are we evaluating this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, the `LatentDirichletAllocation` class in `sklearn` will use the variational bound as an approximation of the log likeihood with a give set of doc-to-topic distributions. There's no doc-completion thing here. \n",
    "\n",
    "This is the basis of `score()`, which internally calculates the \"unnormalized\" topic distribution of the documents, then uses the variational bound to approximate the log likelihood; this in turn is the basis of `perplexity()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did Hannah Wallach say?\n",
    "\n",
    " * Well she's thinking of T topics, and I guess ? words so here component distribution is  $\\Phi \\in \\mathbb{R}^{T \\times ?}$ with prior $\\text{Dir}(\\phi_t; \\beta \\boldsymbol{n})$\n",
    " * For each of the $D$ documents there's a topic distribution $\\theta_d$ with prior $\\text{Dir}(\\theta_d; \\alpha \\boldsymbol{m})$\n",
    "\n",
    "Finally, she notes the Polya identity, allowing the marginalisation of most parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She then moves out into how to evaluate the probability of some held out documents $W$ given some training documents $W'$ which is\n",
    "\n",
    "$$\n",
    "p(W | W') = \\int d\\Phi d\\alpha d\\boldsymbol{m}\n",
    "             \\text{ } p(W | \\Phi, \\alpha, \\boldsymbol{m}) \\text{ } p(\\Phi, \\alpha, \\boldsymbol{m}|W')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to note here is she has already margnalised out $\\Theta$ for the new documents. She assumes you learn the \"global\" parameters -- priors and component distribution -- and then fix these and use them to evaluate the new documents\n",
    "\n",
    "> So we have to think about what we're doing here. A mixture model is a good case. You can just directly evaluate the log likelihood $p(w|\\alpha, \\Phi) = \\sum_k p(w | \\phi_k)p(z=k|\\alpha)$. Or you can determine the posterior over clusters and use that to evaluate... except that it doesn't decompose $p(w|\\alpha, \\Phi) = \\sum_k p(w, z=k|\\alpha, \\Phi) = p(z=k|w, \\alpha, \\Phi)p(w|\\ldots)$. But it seems obvious to see how well you can \"explain\" documents: this is what doc-completion does. Hence it should be introduced in the clustering section. It's also a good metric to use if you want to consider the predictive ability to, e.g. predict hashtags.\n",
    "\n",
    "Now either way, you have to make a choice about your parameters. Are you using the _distribution_ over the parameters, or are you just taking a point estimate?\n",
    "\n",
    "1. Drawing samples from the parameter posterior and taking an average to evaluate the integral, i.e.  $\\mathbb{E}_{p(\\Phi, \\alpha, \\boldsymbol{m}|W')}\\left[ p(W | \\Phi, \\alpha, \\boldsymbol{m}) \\right]$. \n",
    "    * Stick a log in that expectation and you can start thinking about a variational approximation.\n",
    "2. Taking a point estimate of -- I guess $\\Phi, \\alpha, \\boldsymbol{m}$ -- and then use that to approximate\n",
    "\n",
    "The paper is concerned with point estimates. So where's the uncertainty.... Apparently its in $p(\\boldsymbol{w}_d | \\alpha \\boldsymbol{m}, \\Phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is that we've marginalised out $\\theta$ for each of the inference documents. We need this too. If you hold $\\Phi$ fixed (and so let it be found by any inference method), you can use Gibbs sampling to quickly get a distribution over $z$ and thereby, $\\theta$.\n",
    "\n",
    " * This is used by many methods she describes, being: FIXME\n",
    " * There are other methods that do not require this, being: FIXME\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $p(w|\\Phi, a \\boldsymbol{m})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Using Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hence there are two options:\n",
    "\n",
    "Directly sample $\\theta \\sim Dir(\\alpha \\boldsymbol{m})$ and average over all settings. But importance sampling doesn't work well in high-dimensions: it has high-variance, indeed, infinite variance with real-values high-dim values.\n",
    "\n",
    "The other is to choose a proposal distribution and weight such samples in the usual importance-sampling way. The proposal distribution is in fact a method for evaluating the posterior $p(z|w, \\alpha \\boldsymbol{m}, \\Phi)$\n",
    "\n",
    "$$\n",
    "\\theta^0 \\propto \\left(\\alpha \\boldsymbol{m}\\right) \\text{.* } \\Phi_{\\cdot, w_{n}} \n",
    "$$\n",
    "\n",
    "Which is just the prior over topics and the probability of words under each topic, i.e. $p(z = k| w, \\Phi, \\alpha \\boldsymbol{m}) \\propto p(w|Phi, z=k)p(z=k| \\alpha \\boldsymbol{m})$\n",
    "\n",
    "To draw samples, simply iterate\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{for }& s = 0 \\ldots S \\\\\n",
    " & z_n^{(s)} \\sim \\text{Mul}(\\theta^{(s)}, 1) \\\\\n",
    " & \\theta^{(s+1)} \\propto \\left(\\alpha \\boldsymbol{m} + \\sum_{n' \\neq n} \\theta^{(s)} \\text{.* } \\boldsymbol{\\bar{z}}_{n'}\\right) \\Phi_{\\cdot, w_{n}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(Recall that in more normal notation $\\alpha \\boldsymbol{m} = \\boldsymbol{\\alpha}$ and parameterises the prior. Also $z_n$ is the scalar and $\\bar{\\boldsymbol{z}}_n$ is the indicator vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Harmonic Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gibbs sampling to get a _posterior_ distribution over $z_n^s$.\n",
    "\n",
    "Then instead of using that to materlise an estimate of $\\theta$ (WHY), use it directly to figure out $p(w | \\alpha \\boldsymbol{m}, \\Phi)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
