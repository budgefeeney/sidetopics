{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Results\n",
    "\n",
    "**Objective:** Basic illustrative statistics to demonstrate the impact of model type, hyper-parameter optimisation, and evaluation metrics on document-model scoring\n",
    "\n",
    "**Method**: We carry out the following experiments\n",
    "\n",
    " * For three datasets, for MoM/VB, MoM/Gibbs, LDA/VB, LDA/CVB, LDA/CVB0 for K in `{5, 10, 25, 50, 100, 200}` evaluate perplexity as document completion. Create multi-line trend plot. Persist\n",
    " * For three datasets, for HDP with three concentrations, evaluate perplexity as document completion. Create a second trend-plot, overlaid with LDA above\n",
    " * For three datasets, for LDA/VB for K in `{5, 10, 25, 50, 100, 200}` with hyperparam enabled, evaluate perplexity. Plot against LDA without hyper-param enabled.\n",
    " * For three datasets, for LDA/VB, for on K in `$BEST_K` with batch sizes of all, 1, 25, 100, ALL, plot held out perplexity every 10 iterations. \n",
    " * For Reuters and 20News, for K in the usual range, generate scores according to the usual methods\n",
    " \n",
    "\n",
    "**Outstanding Implementation Details:** Next steps\n",
    "\n",
    " * We need to be able to come up with a document-completion ScoringMethod.\n",
    "     * This in turn requires us to throw an exception if `y` or `y_query_state` is supplied.\n",
    "     * It also requires us to do the split in the `score()` method\n",
    " * If we assume the _training_ is the hardest part, then doing the querying for all the other cross-validation scores is easy\n",
    "     * So change the workflow to use a custom split\n",
    "     * and then write code to do the extra scores\n",
    "     * and then write code to process them \n",
    " * Is perplexity sensible. If it _sums_ then it's enormous. Probably best thing to do is to calculate the log-likelihood, and then have a separate function that for a given corpus sums the log-likelihood to get the overall corpus perplexity..\n",
    "     * With the doc-completion split this will take some doing, do we just make an optimistic assumption that things cancel out?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import scipy.sparse as ssp\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Markdown\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "_ = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-7s %(module)s::%(funcName)s() - %(message)s',\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(pathlib.Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sidetopics.model.common import DataSet\n",
    "import sidetopics.model.sklearn.lda_cvb as _lda_cvb\n",
    "import sidetopics.model.sklearn as mytopics\n",
    "\n",
    "importlib.reload(_lda_cvb)\n",
    "importlib.reload(mytopics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0xC0FFEE\n",
    "rd.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = pathlib.Path(pathlib.Path.home().root)\n",
    "WINDOWS_PARTITON = ROOT_DIR / 'media' / 'bfeeney' / 'Blade 15'\n",
    "WINDOWS_HOME_DIR = WINDOWS_PARTITON / 'Users' / 'bryan'\n",
    "\n",
    "GDRIVE_DIR = WINDOWS_HOME_DIR / 'Google Drive'\n",
    "DATASET_DIR_ON_WIN_GDRIVE = GDRIVE_DIR / 'DatasetSSD'\n",
    "DATASET_DIR_ON_MACOS_SSD = ROOT_DIR / 'Volumes' / 'DatasetSSD'\n",
    "\n",
    "if DATASET_DIR_ON_MACOS_SSD.exists():\n",
    "    DATASET_DIR = DATASET_DIR_ON_MACOS_SSD\n",
    "else:\n",
    "    DATASET_DIR = DATASET_DIR_ON_WIN_GDRIVE\n",
    "assert DATASET_DIR.exists(), f\"Cannot find dataset directory {DATASET_DIR}\"\n",
    "    \n",
    "CLEAN_DATASET_DIR = DATASET_DIR / 'words-only'\n",
    "\n",
    "T20_NEWS_DIR = CLEAN_DATASET_DIR / '20news4'\n",
    "NIPS_DIR = CLEAN_DATASET_DIR / 'nips'\n",
    "REUTERS_DIR = CLEAN_DATASET_DIR / 'reuters'\n",
    "\n",
    "TRUMP_WEEKS_DIR = DATASET_DIR / 'TrumpDb'\n",
    "NUS_WIDE_DIR = DATASET_DIR / 'NusWide'\n",
    "\n",
    "CITHEP_DATASET_DIR = DATASET_DIR / 'Arxiv'\n",
    "ACL_DATASET_DIR = DATASET_DIR / 'ACL' / 'ACL.100.clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_COUNTS = [5, 10, 25, 50, 100, 200]\n",
    "BATCH_SIZES = [1, 10, 50, 100, 500, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t20_news = DataSet.from_files(words_file=T20_NEWS_DIR / 'words.pkl')\n",
    "reuters = DataSet.from_files(words_file=REUTERS_DIR / 'words.pkl')\n",
    "acl = DataSet.from_files(words_file=ACL_DATASET_DIR / 'words.pkl')\n",
    "arxiv = DataSet.from_files(words_file=CITHEP_DATASET_DIR / 'words.pkl')\n",
    "nips = DataSet.from_files(words_file=NIPS_DIR / 'words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t20_news.convert_to_dtype(DTYPE)\n",
    "reuters.convert_to_dtype(DTYPE)\n",
    "acl.convert_to_dtype(DTYPE)\n",
    "arxiv.convert_to_dtype(DTYPE)\n",
    "nips.convert_to_dtype(DTYPE)\n",
    "\n",
    "# Maybe shuffle here in order to keep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(dataset: DataSet, min_word_count: int = 25) -> str:\n",
    "    quarts = np.percentile(a=dataset.words.sum(axis=1), q=[0, 25, 50, 75, 100]).astype(np.int32)\n",
    "    quarts_str = ' | '.join(f'{q:,}' for q in quarts)\n",
    "    \n",
    "    doc_min_count = (dataset.doc_lens > min_word_count).sum()\n",
    "    \n",
    "    return f'{dataset.doc_count:,} | {doc_min_count:,} | {int(dataset.word_count):,} | {dataset.words.shape[1]} | {quarts_str}'\n",
    "\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "\n",
    "| Dataset | Document Count | Document > 50 Count | Total Words | Vocabulary Size | DocLen (Min) | DocLen (25) | DocLen (50) | DocLen (75) | DocLen (Max) |\n",
    "| ------- | -------------- | ------------------- | ----------- | --------------- | ------------ | ----------- | ----------- | ----------- | ------------ |\n",
    "| Reuters-21578 | {corpus_stats(reuters)} |\n",
    "| 20-News | {corpus_stats(t20_news)} |\n",
    "| NIPS | {corpus_stats(nips)} |\n",
    "| ACL | {corpus_stats(acl)} |\n",
    "| Arxiv | {corpus_stats(arxiv)} |\n",
    "\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to emphasise here is that we're deliberately chose a mix of datasets with small document lengths and large document lengths to look into overparameterisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 1: Perplexity Across K and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reuters.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel = GridSearchCV(\n",
    "    estimator=mytopics.TopicModel(\n",
    "        kind='LDA_VB',\n",
    "        n_components=10,\n",
    "        seed=RANDOM_SEED,\n",
    "        default_scoring_method=mytopics.ScoreMethod.DocCompletionPerplexityPoint\n",
    "    ),\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=rd.RandomState(seed=0xC0FFEE)),\n",
    "    n_jobs=14,\n",
    "    param_grid={\n",
    "        'kind': ['MOM_VB', 'MOM_GIBBS', 'LDA_GIBBS', 'LDA_CVB', 'LDA_CVB0']\n",
    "    }\n",
    ")\n",
    "\n",
    "gmodel.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model_state_dir = pathlib.Path.cwd() / 'saved_models'\n",
    "if not model_state_dir.exists():\n",
    "    model_state_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_state_dir / 'topic-model-general.pkl', 'wb') as f:\n",
    "    pkl.dump(gmodel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_state_dir / 'topic-model-general.pkl', 'rb') as f:\n",
    "    gmodel_restored = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_restored.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel = GridSearchCV(\n",
    "    estimator=mytopics.WrappedSckitLda(\n",
    "        n_components=10,\n",
    "        seed=RANDOM_SEED,\n",
    "        default_scoring_method=mytopics.ScoreMethod.DocCompletionPerplexityPoint\n",
    "    ),\n",
    "    cv=5,\n",
    "    n_jobs=14,\n",
    "    param_grid={\n",
    "        'kind': ['MOM_VB'],\n",
    "        'n_components': TOPIC_COUNTS\n",
    "    }\n",
    ")\n",
    "\n",
    "gmodel.fit(X)\n",
    "plt.plot(TOPIC_COUNTS, gmodel.cv_results_['mean_test_score'], 's-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [t20_news, reuters, acl, arxiv, nips]:\n",
    "    X = dataset.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Issue 1: MoM vs LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = reuters.cross_valid_split(test_fold_id=0, num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, valid = test.doc_completion_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import HdpTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LatentDirichletAllocation.score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are we evaluating this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, the `LatentDirichletAllocation` class in `sklearn` will use the variational bound as an approximation of the log likeihood with a give set of doc-to-topic distributions. There's no doc-completion thing here. \n",
    "\n",
    "This is the basis of `score()`, which internally calculates the \"unnormalized\" topic distribution of the documents, then uses the variational bound to approximate the log likelihood; this in turn is the basis of `perplexity()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did Hannah Wallach say?\n",
    "\n",
    " * Well she's thinking of T topics, and I guess ? words so here component distribution is  $\\Phi \\in \\mathbb{R}^{T \\times ?}$ with prior $\\text{Dir}(\\phi_t; \\beta \\boldsymbol{n})$\n",
    " * For each of the $D$ documents there's a topic distribution $\\theta_d$ with prior $\\text{Dir}(\\theta_d; \\alpha \\boldsymbol{m})$\n",
    "\n",
    "Finally, she notes the Polya identity, allowing the marginalisation of most parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She then moves out into how to evaluate the probability of some held out documents $W$ given some training documents $W'$ which is\n",
    "\n",
    "$$\n",
    "p(W | W') = \\int d\\Phi d\\alpha d\\boldsymbol{m}\n",
    "             \\text{ } p(W | \\Phi, \\alpha, \\boldsymbol{m}) \\text{ } p(\\Phi, \\alpha, \\boldsymbol{m}|W')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to note here is she has already margnalised out $\\Theta$ for the new documents. She assumes you learn the \"global\" parameters -- priors and component distribution -- and then fix these and use them to evaluate the new documents\n",
    "\n",
    "> So we have to think about what we're doing here. A mixture model is a good case. You can just directly evaluate the log likelihood $p(w|\\alpha, \\Phi) = \\sum_k p(w | \\phi_k)p(z=k|\\alpha)$. Or you can determine the posterior over clusters and use that to evaluate... except that it doesn't decompose $p(w|\\alpha, \\Phi) = \\sum_k p(w, z=k|\\alpha, \\Phi) = p(z=k|w, \\alpha, \\Phi)p(w|\\ldots)$. But it seems obvious to see how well you can \"explain\" documents: this is what doc-completion does. Hence it should be introduced in the clustering section. It's also a good metric to use if you want to consider the predictive ability to, e.g. predict hashtags.\n",
    "\n",
    "Now either way, you have to make a choice about your parameters. Are you using the _distribution_ over the parameters, or are you just taking a point estimate?\n",
    "\n",
    "1. Drawing samples from the parameter posterior and taking an average to evaluate the integral, i.e.  $\\mathbb{E}_{p(\\Phi, \\alpha, \\boldsymbol{m}|W')}\\left[ p(W | \\Phi, \\alpha, \\boldsymbol{m}) \\right]$. \n",
    "    * Stick a log in that expectation and you can start thinking about a variational approximation.\n",
    "2. Taking a point estimate of -- I guess $\\Phi, \\alpha, \\boldsymbol{m}$ -- and then use that to approximate\n",
    "\n",
    "The paper is concerned with point estimates. So where's the uncertainty.... Apparently its in $p(\\boldsymbol{w}_d | \\alpha \\boldsymbol{m}, \\Phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is that we've marginalised out $\\theta$ for each of the inference documents. We need this too. If you hold $\\Phi$ fixed (and so let it be found by any inference method), you can use Gibbs sampling to quickly get a distribution over $z$ and thereby, $\\theta$.\n",
    "\n",
    " * This is used by many methods she describes, being: FIXME\n",
    " * There are other methods that do not require this, being: FIXME\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $p(w|\\Phi, a \\boldsymbol{m})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Using Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hence there are two options:\n",
    "\n",
    "Directly sample $\\theta \\sim Dir(\\alpha \\boldsymbol{m})$ and average over all settings. But importance sampling doesn't work well in high-dimensions: it has high-variance, indeed, infinite variance with real-values high-dim values.\n",
    "\n",
    "The other is to choose a proposal distribution and weight such samples in the usual importance-sampling way. The proposal distribution is in fact a method for evaluating the posterior $p(z|w, \\alpha \\boldsymbol{m}, \\Phi)$\n",
    "\n",
    "$$\n",
    "\\theta^0 \\propto \\left(\\alpha \\boldsymbol{m}\\right) \\text{.* } \\Phi_{\\cdot, w_{n}} \n",
    "$$\n",
    "\n",
    "Which is just the prior over topics and the probability of words under each topic, i.e. $p(z = k| w, \\Phi, \\alpha \\boldsymbol{m}) \\propto p(w|Phi, z=k)p(z=k| \\alpha \\boldsymbol{m})$\n",
    "\n",
    "To draw samples, simply iterate\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{for }& s = 0 \\ldots S \\\\\n",
    " & z_n^{(s)} \\sim \\text{Mul}(\\theta^{(s)}, 1) \\\\\n",
    " & \\theta^{(s+1)} \\propto \\left(\\alpha \\boldsymbol{m} + \\sum_{n' \\neq n} \\theta^{(s)} \\text{.* } \\boldsymbol{\\bar{z}}_{n'}\\right) \\Phi_{\\cdot, w_{n}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(Recall that in more normal notation $\\alpha \\boldsymbol{m} = \\boldsymbol{\\alpha}$ and parameterises the prior. Also $z_n$ is the scalar and $\\bar{\\boldsymbol{z}}_n$ is the indicator vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Harmonic Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gibbs sampling to get a _posterior_ distribution over $z_n^s$.\n",
    "\n",
    "Then instead of using that to materlise an estimate of $\\theta$ (WHY), use it directly to figure out $p(w | \\alpha \\boldsymbol{m}, \\Phi)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
